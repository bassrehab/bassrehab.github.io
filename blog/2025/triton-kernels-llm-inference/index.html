<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="R3nPzqP9nO2aTgysGUiU3t-pw3wi1xuUuKVaMrimuck"> <meta name="msvalidate.01" content="04fec5e0d64b1c9bc788e98f81ee2a78"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference | Subhadip Mitra </title> <meta name="author" content="Subhadip Mitra"> <meta name="description" content="A hands-on exploration of writing custom GPU kernels with OpenAI Triton, going from PyTorch's 11% bandwidth utilization to 88% on RMSNorm."> <meta name="keywords" content="subhadip-mitra,subhadip,google,google-cloud,data-&amp;-analytics,ai-innovations,enterprise-technology-leadership,digital-transformation,machine-learning-models,cloud-technologies,quantum-computing,technology-consulting,singapore,researcher,llm,genai"> <meta property="og:site_name" content="Subhadip Mitra"> <meta property="og:type" content="article"> <meta property="og:title" content="Subhadip Mitra | From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference"> <meta property="og:url" content="https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/"> <meta property="og:description" content="A hands-on exploration of writing custom GPU kernels with OpenAI Triton, going from PyTorch's 11% bandwidth utilization to 88% on RMSNorm."> <meta property="og:image" content="https://subhadipmitra.com/assets/img/og/triton-kernels-llm-inference.png"> <meta property="og:image:width" content="1200"> <meta property="og:image:height" content="630"> <meta property="og:locale" content="en"> <meta property="og:logo" content="https://subhadipmitra.com/assets/img/prof_pic.jpg"> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference"> <meta name="twitter:description" content="A hands-on exploration of writing custom GPU kernels with OpenAI Triton, going from PyTorch's 11% bandwidth utilization to 88% on RMSNorm."> <meta name="twitter:image" content="https://subhadipmitra.com/assets/img/og/triton-kernels-llm-inference.png"> <meta property="article:published_time" content="2025-06-15T10:30:00+00:00"> <meta property="article:author" content="Subhadip Mitra"> <meta property="article:tag" content="machine-learning"> <meta property="article:tag" content="llm"> <meta property="article:tag" content="inference"> <meta property="article:tag" content="optimization"> <meta property="article:tag" content="triton"> <meta property="article:tag" content="gpu"> <meta property="article:section" content="AI Infrastructure"> <meta property="article:section" content="Research"> <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/"
        },
        "headline": "From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference",
        "description": "A hands-on exploration of writing custom GPU kernels with OpenAI Triton, going from PyTorch&#39;s 11% bandwidth utilization to 88% on RMSNorm.",
        "url": "https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/",
        "datePublished": "2025-06-15T10:30:00+00:00",
        
        "dateModified": "2025-06-15T10:30:00+00:00",
        
        "wordCount": 2131,
        "timeRequired": "PT11M",
        
        "image": "https://subhadipmitra.com/assets/img/social_preview.png",
        
        
        "keywords": "machine-learning, llm, inference, optimization, triton, gpu",
        
        
        "articleSection": "AI Infrastructure",
        
        "author": {
            "@type": "Person",
            "name": "Subhadip Mitra",
            "url": "https://subhadipmitra.com",
            "jobTitle": "Data and Analytics Manager, Site Lead Southeast Asia",
            "worksFor": {
                "@type": "Organization",
                "name": "Google Cloud"
            },
            "sameAs": [
                "https://linkedin.com/in/subhadip-mitra",
                "https://github.com/bassrehab",
                "https://twitter.com/bassrehab"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "Subhadip Mitra",
            "url": "https://subhadipmitra.com",
            "logo": {
                "@type": "ImageObject",
                "url": "https://subhadipmitra.com/assets/img/prof_pic.jpg"
            }
        },
        "isPartOf": {
            "@type": "Blog",
            "name": "binary breakthroughs",
            "url": "https://subhadipmitra.com/blog/"
        }
        
    }
  </script> <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [
            {
                "@type": "ListItem",
                "position": 1,
                "name": "Home",
                "item": "https://subhadipmitra.com"
            },
            {
                "@type": "ListItem",
                "position": 2,
                "name": "Blog",
                "item": "https://subhadipmitra.com/blog/"
            },
            
            {
                "@type": "ListItem",
                "position": 3,
                "name": "Ai infrastructure",
                "item": "https://subhadipmitra.com/blog/category/ai-infrastructure"
            },
            {
                "@type": "ListItem",
                "position": 4,
                "name": "From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference",
                "item": "https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/"
            }
            
        ]
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&amp;family=Inter+Tight:ital,wght@0,100..900;1,100..900&amp;family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?058310854f7b3c920f894a1d42c3c47b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/"> <script src="/assets/js/theme.js?33a804e644f2507dbddc853404eccc7a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Subhadip</span> Mitra </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/now/">now </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/repositories/">repositories</a> <a class="dropdown-item " href="/archive/">archive</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/contact/">contact</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/license/">licenses</a> <a class="dropdown-item " href="/privacy/">privacy</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://upir.subhadipmitra.com" rel="external nofollow noopener" target="_blank">docs: UPIR</a> <a class="dropdown-item " href="https://ai-metacognition-toolkit.subhadipmitra.com/" rel="external nofollow noopener" target="_blank">docs: AI Metacognition</a> <a class="dropdown-item " href="https://symmetry.subhadipmitra.com/" rel="external nofollow noopener" target="_blank">symmetry</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference</h1> <p class="post-meta"> Created on June 15, 2025 by Subhadip Mitra </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/inference"> <i class="fa-solid fa-hashtag fa-sm"></i> inference</a>   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>   <a href="/blog/tag/triton"> <i class="fa-solid fa-hashtag fa-sm"></i> triton</a>   <a href="/blog/tag/gpu"> <i class="fa-solid fa-hashtag fa-sm"></i> gpu</a>   ·   <a href="/blog/category/ai-infrastructure"> <i class="fa-solid fa-tag fa-sm"></i> AI Infrastructure</a>   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a> </p> </header> <div class="audio-reader-container"> <div class="audio-reader-compact"> <button class="audio-trigger" id="audio-trigger" aria-label="Open audio player"> <i class="fas fa-headphones"></i> <span>Listen to article</span> <i class="fas fa-chevron-down audio-trigger-icon"></i> </button> <div class="audio-player-expanded" id="audio-player-expanded"> <div class="audio-player-body"> <div class="audio-info-bar"> <i class="fas fa-info-circle"></i> <span>Browser text-to-speech • Quality varies by device</span> </div> <div class="audio-controls-row"> <button class="audio-btn audio-btn-play" id="tts-play" aria-label="Play"> <i class="fas fa-play"></i> <span id="play-text">Play</span> </button> <button class="audio-btn audio-btn-pause" id="tts-pause" style="display: none;" aria-label="Pause"> <i class="fas fa-pause"></i> <span>Pause</span> </button> <button class="audio-btn audio-btn-stop" id="tts-stop" aria-label="Stop"> <i class="fas fa-stop"></i> </button> <div class="audio-speed-control"> <label for="tts-rate" class="audio-speed-label"> <i class="fas fa-gauge-high"></i> <span id="tts-rate-value">1.0x</span> </label> <input id="tts-rate" type="range" min="0.5" max="2" step="0.1" value="1" class="audio-speed-slider"> </div> </div> <div class="audio-voice-row"> <label for="tts-voice" class="audio-voice-label"> <i class="fas fa-microphone"></i> <span>Voice:</span> </label> <select id="tts-voice" class="audio-voice-select"> <option value="">Loading voices...</option> </select> </div> <div class="audio-progress-section"> <div class="audio-progress-info"> <span><i class="fas fa-circle-notch"></i> Progress</span> <span id="tts-progress">0%</span> </div> <div class="progress" style="height: 4px;"> <div class="progress-bar" id="tts-progress-bar" role="progressbar" style="width: 0%" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100"></div> </div> </div> </div> </div> </div> </div> <style>.audio-reader-container{margin:1.5rem 0;font-family:var(--global-font-family,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,sans-serif)}.audio-reader-compact{position:relative}.audio-trigger{display:inline-flex;align-items:center;gap:.5rem;padding:.5rem 1rem;background:var(--global-code-bg-color,#f8f9fa);border:1px solid var(--global-divider-color,#dee2e6);border-radius:.375rem;color:var(--global-text-color,#212529);font-size:.875rem;font-weight:500;cursor:pointer;transition:all .2s ease;font-family:inherit}.audio-trigger:hover{background:var(--global-bg-color,#fff);border-color:var(--global-theme-color,#0d6efd);color:var(--global-theme-color,#0d6efd);box-shadow:0 .125rem .25rem rgba(0,0,0,0.075)}.audio-trigger i:first-child{font-size:1rem;color:var(--global-theme-color,#0d6efd)}.audio-trigger-icon{font-size:.75rem;margin-left:.25rem;transition:transform .3s ease;color:var(--global-text-color-light,#6c757d)}.audio-trigger.active .audio-trigger-icon{transform:rotate(180deg)}.audio-trigger:focus{outline:2px solid var(--global-theme-color,#0d6efd);outline-offset:2px}.audio-player-expanded{display:none;margin-top:.75rem;background:var(--global-bg-color,#fff);border:1px solid var(--global-divider-color,#dee2e6);border-radius:.5rem;box-shadow:0 .25rem .75rem rgba(0,0,0,0.05);animation:slideDown .3s ease}.audio-player-expanded.show{display:block}@keyframes slideDown{from{opacity:0;transform:translateY(-0.5rem)}to{opacity:1;transform:translateY(0)}}.audio-player-body{padding:1rem}.audio-info-bar{display:flex;align-items:center;gap:.5rem;padding:.625rem .75rem;background:var(--global-code-bg-color,#f8f9fa);border-left:2px solid var(--global-theme-color,#0d6efd);border-radius:.25rem;font-size:.8125rem;color:var(--global-text-color-light,#6c757d);margin-bottom:1rem}.audio-info-bar i{color:var(--global-theme-color,#0d6efd);font-size:.875rem}.audio-controls-row{display:flex;align-items:center;gap:.5rem;margin-bottom:.875rem;flex-wrap:wrap}.audio-btn{display:inline-flex;align-items:center;gap:.375rem;padding:.375rem .875rem;border:1px solid transparent;border-radius:.375rem;font-size:.875rem;font-weight:500;cursor:pointer;transition:all .15s ease;font-family:inherit;color:#fff}.audio-btn i{font-size:.8125rem}.audio-btn span{color:#fff}.audio-btn:hover:not(:disabled){transform:translateY(-1px);box-shadow:0 .125rem .375rem rgba(0,0,0,0.15)}.audio-btn:active:not(:disabled){transform:translateY(0)}.audio-btn:disabled{opacity:.6;cursor:not-allowed}.audio-btn-play{background:var(--global-theme-color,#0d6efd);border-color:var(--global-theme-color,#0d6efd)}.audio-btn-play:hover:not(:disabled){background:color-mix(in srgb,var(--global-theme-color,#0d6efd) 85%,black);border-color:color-mix(in srgb,var(--global-theme-color,#0d6efd) 80%,black)}.audio-btn-play.playing{background:#198754;border-color:#198754}.audio-btn-pause{background:#6c757d;border-color:#6c757d}.audio-btn-pause:hover:not(:disabled){background:#5c636a;border-color:#565e64}.audio-btn-stop{background:transparent;color:#dc3545;border-color:#dc3545;padding:.375rem .625rem}.audio-btn-stop:hover:not(:disabled){background:#dc3545;color:#fff;border-color:#dc3545}.audio-speed-control{display:flex;align-items:center;gap:.5rem;margin-left:auto}.audio-speed-label{display:flex;align-items:center;gap:.375rem;font-size:.8125rem;font-weight:500;color:var(--global-text-color,#212529);margin:0;white-space:nowrap}.audio-speed-label i{color:var(--global-theme-color,#0d6efd);font-size:.875rem}.audio-speed-slider{width:80px;height:4px;-webkit-appearance:none;appearance:none;background:var(--global-divider-color,#dee2e6);border-radius:2px;outline:0}.audio-speed-slider::-webkit-slider-thumb{-webkit-appearance:none;appearance:none;width:14px;height:14px;border-radius:50%;background:var(--global-theme-color,#0d6efd);cursor:pointer;transition:transform .15s ease}.audio-speed-slider::-webkit-slider-thumb:hover{transform:scale(1.2)}.audio-speed-slider::-moz-range-thumb{width:14px;height:14px;border-radius:50%;background:var(--global-theme-color,#0d6efd);border:0;cursor:pointer;transition:transform .15s ease}.audio-speed-slider::-moz-range-thumb:hover{transform:scale(1.2)}.audio-voice-row{display:flex;align-items:center;gap:.75rem;margin-bottom:.875rem}.audio-voice-label{display:flex;align-items:center;gap:.375rem;font-size:.8125rem;font-weight:500;color:var(--global-text-color,#212529);margin:0;white-space:nowrap}.audio-voice-label i{color:var(--global-theme-color,#0d6efd);font-size:.875rem}.audio-voice-select{flex:1;padding:.375rem .75rem;border:1px solid var(--global-divider-color,#dee2e6);border-radius:.375rem;font-size:.8125rem;background:var(--global-bg-color,#fff);color:var(--global-text-color,#212529);cursor:pointer;transition:border-color .15s ease}.audio-voice-select:hover{border-color:var(--global-theme-color,#0d6efd)}.audio-voice-select:focus{outline:0;border-color:var(--global-theme-color,#0d6efd);box-shadow:0 0 0 .2rem rgba(13,110,253,0.25)}.audio-progress-section{margin-top:.75rem;padding-top:.75rem;border-top:1px solid var(--global-divider-color,#e9ecef)}
.audio-progress-info{display:flex;justify-content:space-between;align-items:center;font-size:.8125rem;color:var(--global-text-color-light,#6c757d);margin-bottom:.5rem}.audio-progress-info i{margin-right:.25rem;color:var(--global-theme-color,#0d6efd);font-size:.75rem}.progress{border-radius:2px;overflow:hidden;background:var(--global-divider-color,#e9ecef)}.progress-bar{background:var(--global-theme-color,#0d6efd);transition:width .6s ease}[data-theme="dark"] .audio-trigger,body.dark-mode .audio-trigger{background:rgba(255,255,255,0.05);border-color:var(--global-divider-color,#454545);color:var(--global-text-color,#e9ecef)}[data-theme="dark"] .audio-trigger:hover,body.dark-mode .audio-trigger:hover{background:rgba(255,255,255,0.08)}[data-theme="dark"] .audio-player-expanded,body.dark-mode .audio-player-expanded{background:var(--global-bg-color,#1e1e1e);border-color:var(--global-divider-color,#454545)}[data-theme="dark"] .audio-info-bar,body.dark-mode .audio-info-bar{background:rgba(255,255,255,0.03)}[data-theme="dark"] .audio-voice-select,body.dark-mode .audio-voice-select{background:var(--global-bg-color,#1e1e1e);border-color:var(--global-divider-color,#454545);color:var(--global-text-color,#e9ecef)}[data-theme="dark"] .progress,body.dark-mode .progress{background:rgba(255,255,255,0.1)}@media(max-width:576px){.audio-trigger span{display:none}.audio-controls-row{flex-wrap:wrap}.audio-btn span{display:none}.audio-btn{padding:.5rem .75rem}.audio-btn i{font-size:1rem}.audio-speed-control{width:100%;margin-left:0;margin-top:.5rem}.audio-speed-slider{flex:1}.audio-voice-row{flex-direction:column;align-items:stretch}}@media print{.audio-reader-container{display:none!important}}@media(prefers-reduced-motion:reduce){.audio-trigger,.audio-btn,.progress-bar,.audio-player-expanded{transition:none;animation:none}}</style> <script>
(function() {
  'use strict';
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', init);
  } else {
    init();
  }
  
  function init() {
    if (!('speechSynthesis' in window)) {
      document.querySelector('.audio-reader-container').innerHTML = `
        <div class="audio-reader-compact">
          <div class="alert alert-warning d-flex align-items-center" role="alert" style="font-size: 0.875rem;">
            <i class="fas fa-exclamation-triangle me-2"></i>
            <div>Text-to-speech not supported in this browser.</div>
          </div>
        </div>
      `;
      return;
    }

    const synth = window.speechSynthesis;
    let utterance = null;
    let isPaused = false;
    
    const triggerBtn = document.getElementById('audio-trigger');
    const expandedPlayer = document.getElementById('audio-player-expanded');
    const playBtn = document.getElementById('tts-play');
    const pauseBtn = document.getElementById('tts-pause');
    const stopBtn = document.getElementById('tts-stop');
    const voiceSelect = document.getElementById('tts-voice');
    const rateSlider = document.getElementById('tts-rate');
    const rateValue = document.getElementById('tts-rate-value');
    const progressText = document.getElementById('tts-progress');
    const progressBar = document.getElementById('tts-progress-bar');
    const playText = document.getElementById('play-text');

    const possibleSelectors = [
      '.post-content',
      'article .post-content',
      '.post .post-content',
      'article.post',
      '.post',
      'article',
      'main article',
      'main .container',
      'main',
      '.l-main',
      '.content'
    ];
    
    let articleElement = null;
    for (const selector of possibleSelectors) {
      articleElement = document.querySelector(selector);
      if (articleElement) break;
    }
    
    if (!articleElement) {
      console.warn('Audio Reader: Could not find article content');
      return;
    }
    
    const clonedContent = articleElement.cloneNode(true);
    const elementsToRemove = [
      'pre', 'code', 'script', 'style', 'nav', '.audio-reader-container',
      '.citation', '.giscus', '.utterances', '.navigation', '.pagination',
      '.social', '.share', 'header', 'footer', '.header', '.footer', '.sidebar'
    ];
    
    elementsToRemove.forEach(selector => {
      clonedContent.querySelectorAll(selector).forEach(el => el.remove());
    });
    
    const articleText = clonedContent.innerText
      .replace(/\s+/g, ' ')
      .replace(/\n{3,}/g, '\n\n')
      .trim();
    
    if (!articleText || articleText.length < 100) {
      console.warn('Audio Reader: Article too short');
      return;
    }

    // Toggle player
    triggerBtn.addEventListener('click', function() {
      expandedPlayer.classList.toggle('show');
      this.classList.toggle('active');
      localStorage.setItem('audioPlayerExpanded', expandedPlayer.classList.contains('show'));
    });

    // Load voices
    function loadVoices() {
      const voices = synth.getVoices();
      if (voices.length === 0) {
        setTimeout(loadVoices, 100);
        return;
      }
      
      voiceSelect.innerHTML = '<option value="">Default Voice</option>';
      
      const englishVoices = voices.filter(v => v.lang.startsWith('en'));
      const preferredVoices = englishVoices.filter(v => 
        v.name.includes('Google') || v.name.includes('Microsoft') ||
        v.name.includes('Premium') || v.name.includes('Enhanced') || v.name.includes('Natural')
      );
      const standardEnglishVoices = englishVoices.filter(v => !preferredVoices.includes(v));
      const otherVoices = voices.filter(v => !v.lang.startsWith('en'));
      
      preferredVoices.forEach((voice) => {
        const option = document.createElement('option');
        option.value = voices.indexOf(voice);
        option.textContent = `${voice.name} (${voice.lang})`;
        voiceSelect.appendChild(option);
      });
      
      if (standardEnglishVoices.length > 0) {
        const optgroup = document.createElement('optgroup');
        optgroup.label = 'Other English';
        standardEnglishVoices.forEach((voice) => {
          const option = document.createElement('option');
          option.value = voices.indexOf(voice);
          option.textContent = `${voice.name} (${voice.lang})`;
          optgroup.appendChild(option);
        });
        voiceSelect.appendChild(optgroup);
      }
      
      if (otherVoices.length > 0) {
        const optgroup = document.createElement('optgroup');
        optgroup.label = 'Other Languages';
        otherVoices.forEach((voice) => {
          const option = document.createElement('option');
          option.value = voices.indexOf(voice);
          option.textContent = `${voice.name} (${voice.lang})`;
          optgroup.appendChild(option);
        });
        voiceSelect.appendChild(optgroup);
      }
    }

    loadVoices();
    if (synth.addEventListener) {
      synth.addEventListener('voiceschanged', loadVoices);
    }

    function updateProgress(current, total) {
      const percent = Math.min(100, Math.round((current / total) * 100));
      progressText.textContent = `${percent}%`;
      progressBar.style.width = `${percent}%`;
      progressBar.setAttribute('aria-valuenow', percent);
    }

    function updatePlayButton(isPlaying) {
      const icon = playBtn.querySelector('i');
      if (isPlaying) {
        icon.className = 'fas fa-circle-dot';
        playText.textContent = 'Playing';
        playBtn.classList.add('playing');
      } else {
        icon.className = 'fas fa-play';
        playText.textContent = 'Play';
        playBtn.classList.remove('playing');
      }
    }

    function speak() {
      if (isPaused) {
        synth.resume();
        isPaused = false;
        updatePlayButton(true);
        pauseBtn.style.display = 'inline-flex';
        return;
      }

      synth.cancel();
      utterance = new SpeechSynthesisUtterance(articleText);
      
      if (voiceSelect.value) {
        const voices = synth.getVoices();
        utterance.voice = voices[parseInt(voiceSelect.value)];
      }
      
      utterance.rate = parseFloat(rateSlider.value);
      utterance.pitch = 1;
      utterance.volume = 1;
      
      utterance.onstart = function() {
        updatePlayButton(true);
        pauseBtn.style.display = 'inline-flex';
        playBtn.disabled = false;
      };
      
      utterance.onend = function() {
        updatePlayButton(false);
        pauseBtn.style.display = 'none';
        updateProgress(100, 100);
        setTimeout(() => updateProgress(0, 100), 1000);
      };
      
      utterance.onboundary = function(event) {
        updateProgress(event.charIndex, articleText.length);
      };

      utterance.onerror = function(event) {
        if (event.error !== 'interrupted' && event.error !== 'canceled') {
          alert('Playback error. Try a different voice.');
        }
        updatePlayButton(false);
        pauseBtn.style.display = 'none';
      };

      playBtn.disabled = true;
      synth.speak(utterance);
    }

    function pause() {
      if (synth.speaking && !isPaused) {
        synth.pause();
        isPaused = true;
        const icon = playBtn.querySelector('i');
        icon.className = 'fas fa-play';
        playText.textContent = 'Resume';
        playBtn.classList.remove('playing');
        pauseBtn.style.display = 'none';
      }
    }

    function stop() {
      synth.cancel();
      isPaused = false;
      updatePlayButton(false);
      pauseBtn.style.display = 'none';
      updateProgress(0, 100);
    }

    playBtn.addEventListener('click', speak);
    pauseBtn.addEventListener('click', pause);
    stopBtn.addEventListener('click', stop);
    
    rateSlider.addEventListener('input', function(e) {
      rateValue.textContent = `${parseFloat(e.target.value).toFixed(1)}x`;
    });

    window.addEventListener('beforeunload', function() {
      synth.cancel();
    });

    if (localStorage.getItem('audioPlayerExpanded') === 'true') {
      setTimeout(() => {
        expandedPlayer.classList.add('show');
        triggerBtn.classList.add('active');
      }, 100);
    }
  }
})();
</script> <article class="post-content"> <div id="markdown-content"> <p>In my <a href="/blog/2025/making-llm-faster/">previous post on speculative decoding</a>, I kept hammering on one point: LLM inference is memory-bound. The GPU sits idle 98% of the time, waiting for weights to load from memory. Speculative decoding helps by verifying multiple tokens per memory load.</p> <p>But that got me thinking - what about making each individual operation faster? If we’re memory-bound, surely we can do better than PyTorch’s default kernels?</p> <p>Turns out, we can. A lot better. I spent a few weeks writing custom kernels in <a href="https://triton-lang.org/" rel="external nofollow noopener" target="_blank">OpenAI Triton</a> and the results surprised me. PyTorch’s RMSNorm achieves 11% of peak memory bandwidth. My Triton version hits 88%. That’s an 8x speedup on a single operation.</p> <p>This post walks through what I learned. The code is at <a href="https://github.com/bassrehab/triton-kernels" rel="external nofollow noopener" target="_blank">github.com/bassrehab/triton-kernels</a>.</p> <h2 id="why-bother-with-custom-kernels">Why Bother with Custom Kernels?</h2> <p>Quick recap from the <a href="/blog/2025/making-llm-faster/#visualizing-the-bottleneck-roofline-analysis">speculative decoding post</a>: a 7B parameter model in FP16 is 14GB. On an A100 with 2TB/s bandwidth, loading those weights takes ~7ms. The actual math? Maybe 0.1ms.</p> <p>So why is PyTorch leaving 89% of bandwidth on the table for something as simple as RMSNorm?</p> <p>Three reasons:</p> <ol> <li> <p><strong>Kernel launch overhead.</strong> PyTorch dispatches multiple small CUDA kernels. Each launch has ~5-10μs overhead. For small operations, this dominates.</p> </li> <li> <p><strong>Intermediate tensors.</strong> PyTorch materializes intermediate results to GPU memory, then reads them back. More memory traffic = slower.</p> </li> <li> <p><strong>Generic implementations.</strong> PyTorch kernels handle every edge case. Custom kernels can be optimized for specific shapes and dtypes.</p> </li> </ol> <p>Triton lets you write fused kernels that avoid all three problems. And unlike raw CUDA, you don’t need to think about warps, shared memory tiling, or register allocation. Triton handles that.</p> <h2 id="the-kernels-i-built">The Kernels I Built</h2> <p>I focused on four operations common in modern LLMs:</p> <table> <thead> <tr> <th>Kernel</th> <th>What it does</th> <th>Speedup</th> </tr> </thead> <tbody> <tr> <td>RMSNorm</td> <td>Root mean square normalization</td> <td><strong>8.1x</strong></td> </tr> <tr> <td>RMSNorm + Residual (fused)</td> <td>Normalize after residual add</td> <td><strong>6.0x</strong></td> </tr> <tr> <td>SwiGLU (fused)</td> <td>Gated activation (LLaMA-style)</td> <td><strong>1.6x</strong></td> </tr> <tr> <td>INT8 GEMM</td> <td>Quantized matrix multiply</td> <td>~1.0x (2x memory savings)</td> </tr> </tbody> </table> <p>Let me walk through the most interesting one - RMSNorm - to show how Triton optimization works.</p> <h2 id="deep-dive-rmsnorm">Deep Dive: RMSNorm</h2> <p>RMSNorm is dead simple mathematically:</p> \[y = \frac{x}{\sqrt{\frac{1}{n}\sum x^2 + \epsilon}} \cdot \gamma\] <p>That’s it. Compute the root mean square, divide, scale by a learned weight. Every transformer uses this (or LayerNorm, which is similar).</p> <h3 id="the-pytorch-version">The PyTorch Version</h3> <p>Here’s how you’d write it in PyTorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rmsnorm_pytorch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">x_normed</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_normed</span> <span class="o">*</span> <span class="n">weight</span>
</code></pre></div></div> <p>Three lines. Clean. But let’s count the memory operations:</p> <ol> <li> <code class="language-plaintext highlighter-rouge">x.pow(2)</code> - read x, write x²</li> <li> <code class="language-plaintext highlighter-rouge">.mean()</code> - read x², write variance</li> <li> <code class="language-plaintext highlighter-rouge">torch.rsqrt()</code> - read variance, write rsqrt</li> <li> <code class="language-plaintext highlighter-rouge">x * rsqrt</code> - read x again, read rsqrt, write normalized</li> <li> <code class="language-plaintext highlighter-rouge">* weight</code> - read normalized, read weight, write output</li> </ol> <p>That’s reading x twice, plus a bunch of intermediate tensors. For a tensor of shape <code class="language-plaintext highlighter-rouge">(batch, seq, hidden)</code>, we’re moving way more data than necessary.</p> <h3 id="the-triton-version">The Triton Version</h3> <p>Here’s the key insight: we can compute everything in one pass. Read x once, accumulate the sum of squares in registers, compute the normalization factor, write the output. One read, one write.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@triton.jit</span>
<span class="k">def</span> <span class="nf">rmsnorm_kernel</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">,</span>
    <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Each program handles one row
</span>    <span class="n">row</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Pointers for this row
</span>    <span class="n">X_row</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">row</span> <span class="o">*</span> <span class="n">stride</span>
    <span class="n">Y_row</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">row</span> <span class="o">*</span> <span class="n">stride</span>

    <span class="c1"># Accumulate sum of squares in registers (FP32 for precision)
</span>    <span class="n">sum_sq</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">BLOCK_SIZE</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">off</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">off</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">n_cols</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">X_row</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">sum_sq</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>

    <span class="c1"># Compute normalization factor
</span>    <span class="n">mean_sq</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">sum_sq</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_cols</span>
    <span class="n">rstd</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">mean_sq</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

    <span class="c1"># Second pass: normalize and write output
</span>    <span class="k">for</span> <span class="n">off</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">off</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">n_cols</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">X_row</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mf">1.0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">rstd</span> <span class="o">*</span> <span class="n">w</span>
        <span class="n">tl</span><span class="p">.</span><span class="nf">store</span><span class="p">(</span><span class="n">Y_row</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">),</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div></div> <p>A few things to notice:</p> <ol> <li> <p><strong>Two passes, not five.</strong> We read x twice (once for variance, once for normalization), but no intermediate tensors hit memory.</p> </li> <li> <p><strong>FP32 accumulation.</strong> The sum of squares uses float32 to avoid precision loss. The output is still float16.</p> </li> <li> <p><strong>Block-based processing.</strong> Triton processes BLOCK_SIZE elements at a time. The compiler figures out how to map this to GPU threads.</p> </li> </ol> <h3 id="the-results">The Results</h3> <p>On an A100 with LLaMA-7B dimensions (hidden_dim=4096, seq_len=2048):</p> <table> <thead> <tr> <th>Implementation</th> <th>Latency</th> <th>Bandwidth</th> <th>% of Peak</th> </tr> </thead> <tbody> <tr> <td>PyTorch</td> <td>0.30 ms</td> <td>168 GB/s</td> <td>11%</td> </tr> <tr> <td>Triton</td> <td>0.04 ms</td> <td>1365 GB/s</td> <td>88%</td> </tr> </tbody> </table> <p>8.1x faster. And we’re hitting 88% of the A100’s theoretical peak bandwidth (1555 GB/s). That’s about as good as it gets for a memory-bound operation.</p> <h2 id="fusion-the-real-win">Fusion: The Real Win</h2> <p>RMSNorm is fast, but the bigger win comes from fusion. In a transformer block, you typically see:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Residual connection + normalization
</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span> <span class="o">+</span> <span class="n">residual</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="nf">rmsnorm</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
</code></pre></div></div> <p>PyTorch runs these as separate kernels:</p> <ol> <li>Read hidden, read residual, write sum</li> <li>Read sum, compute norm, write output</li> </ol> <p>With Triton, we fuse them into one kernel:</p> <ol> <li>Read hidden, read residual, compute norm, write output</li> </ol> <p>One less round-trip to memory. The fused kernel runs at 6.0x the speed of PyTorch’s separate operations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/fused-kernel-480.webp 480w,/assets/img/blog/fused-kernel-800.webp 800w,/assets/img/blog/fused-kernel-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/fused-kernel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The red boxes (W1, H2) in PyTorch are unnecessary memory operations that fusion eliminates.</p> <h2 id="swiglu-another-fusion-opportunity">SwiGLU: Another Fusion Opportunity</h2> <p>Modern LLMs like LLaMA use SwiGLU instead of ReLU:</p> \[\text{SwiGLU}(x, W, V) = (\text{SiLU}(xW)) \odot (xV)\] <p>In PyTorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gate</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># Write intermediate
</span><span class="n">up</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">V</span>            <span class="c1"># Write intermediate
</span><span class="n">output</span> <span class="o">=</span> <span class="n">gate</span> <span class="o">*</span> <span class="n">up</span>    <span class="c1"># Read both, write output
</span></code></pre></div></div> <p>In Triton, we fuse the final multiply:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># After the matmuls...
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">silu</span><span class="p">(</span><span class="n">gate</span><span class="p">)</span> <span class="o">*</span> <span class="n">up</span>  <span class="c1"># One kernel, no intermediates
</span></code></pre></div></div> <p>The speedup here is more modest (1.6x) because the matmuls dominate. But it’s still free performance.</p> <h2 id="int8-quantization-memory-savings-over-speed">INT8 Quantization: Memory Savings Over Speed</h2> <p>I also implemented INT8 matrix multiplication. The speedup is… basically nothing. Maybe 1.04x. But that’s not the point.</p> <p>The point is <strong>memory savings</strong>. INT8 weights are half the size of FP16. For a 70B model, that’s the difference between fitting in one GPU or needing two.</p> <p>The catch is that you need to dequantize on-the-fly:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dequantize-480.webp 480w,/assets/img/blog/dequantize-800.webp 800w,/assets/img/blog/dequantize-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/dequantize.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The dequantization adds overhead, which is why the speedup is minimal. But for memory-constrained deployments, it’s worth it.</p> <details> <summary><strong>Why not use INT8 activations too? (click to expand)</strong></summary> Full INT8 inference (W8A8) uses the GPU's tensor cores for int8 matmul, which should be faster. But there's a catch: you need to quantize the activations at runtime. For each matmul, you'd need to: 1. Find the max absolute value of the activations 2. Compute a scale factor 3. Quantize to int8 4. Do the int8 matmul 5. Dequantize the output Steps 1-3 add overhead that often outweighs the tensor core speedup, especially for small batch sizes. W8A16 (int8 weights, fp16 activations) avoids this by only quantizing weights once at load time. For more details, see my [INT8 investigation notes](https://github.com/bassrehab/triton-kernels/blob/main/docs/INT8_GEMM_INVESTIGATION.md). </details> <h2 id="lessons-learned">Lessons Learned</h2> <h3 id="1-measure-bandwidth-not-flops">1. Measure bandwidth, not FLOPS</h3> <p>The number that matters for memory-bound operations is GB/s, not TFLOPS. My RMSNorm kernel does barely any floating-point math - it’s almost entirely memory traffic. Measuring FLOPS would be misleading.</p> <h3 id="2-pytorch-overhead-is-real">2. PyTorch overhead is real</h3> <p>For small operations, kernel launch overhead dominates. Fusing operations isn’t just about reducing memory traffic - it’s also about launching fewer kernels.</p> <h3 id="3-triton-is-surprisingly-accessible">3. Triton is surprisingly accessible</h3> <p>I expected writing GPU kernels to be painful. Triton abstracts away most of the hard parts - thread indexing, shared memory, register allocation. You think in terms of blocks and masks, not warps and occupancy.</p> <p>That said, there’s still a learning curve. The <a href="https://triton-lang.org/main/getting-started/tutorials/index.html" rel="external nofollow noopener" target="_blank">Triton tutorials</a> are good. Horace He’s <a href="https://horace.io/brrr_intro.html" rel="external nofollow noopener" target="_blank">GPU optimization guide</a> is essential background.</p> <h3 id="4-dont-roll-your-own-for-production">4. Don’t roll your own for production</h3> <p>These are educational implementations. For real deployments, use <a href="https://github.com/Dao-AILab/flash-attention" rel="external nofollow noopener" target="_blank">FlashAttention</a>, <a href="https://github.com/vllm-project/vllm" rel="external nofollow noopener" target="_blank">vLLM</a>, or <a href="https://github.com/NVIDIA/TensorRT-LLM" rel="external nofollow noopener" target="_blank">TensorRT-LLM</a>. They’ve solved problems I haven’t touched - multi-query attention, paged KV-cache, speculative decoding integration, etc.</p> <p>But if you want to understand <em>why</em> those libraries are fast, writing your own kernels is a great exercise.</p> <h2 id="whats-next">What’s Next</h2> <p>I want to tackle fused attention at some point. FlashAttention is the gold standard, but it’s complex - online softmax, block-sparse patterns, backward pass. Not a weekend project.</p> <p>The more immediate gap in my repo is end-to-end integration. Right now these are standalone kernels. Wiring them into an actual model (say, LLaMA inference) would be a good next step.</p> <h2 id="code">Code</h2> <p>Everything is on GitHub: <a href="https://github.com/bassrehab/triton-kernels" rel="external nofollow noopener" target="_blank">github.com/bassrehab/triton-kernels</a></p> <p>The main files:</p> <ul> <li> <a href="https://github.com/bassrehab/triton-kernels/blob/main/triton_kernels/rmsnorm.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">triton_kernels/rmsnorm.py</code></a> - RMSNorm + fused residual</li> <li> <a href="https://github.com/bassrehab/triton-kernels/blob/main/triton_kernels/swiglu.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">triton_kernels/swiglu.py</code></a> - SwiGLU activation</li> <li> <a href="https://github.com/bassrehab/triton-kernels/blob/main/triton_kernels/quantized_matmul.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">triton_kernels/quantized_matmul.py</code></a> - INT8 GEMM</li> <li> <a href="https://github.com/bassrehab/triton-kernels/tree/main/benchmarks" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">benchmarks/</code></a> - Benchmark suite with roofline analysis</li> </ul> <p>There’s also a detailed <a href="https://github.com/bassrehab/triton-kernels/blob/main/docs/ROOFLINE_ANALYSIS.md" rel="external nofollow noopener" target="_blank">ROOFLINE_ANALYSIS.md</a> that explains the performance characteristics of each kernel.</p> <h2 id="takeaways">Takeaways</h2> <p>If you’re doing LLM inference optimization, the mental model shift is important: stop thinking about FLOPS and start thinking about memory bandwidth. Most operations are memory-bound, and the win comes from:</p> <ol> <li> <strong>Fusion</strong> - fewer round-trips to memory</li> <li> <strong>Quantization</strong> - less data to move</li> <li> <strong>Efficient access patterns</strong> - maximize bandwidth utilization</li> </ol> <p>PyTorch is leaving a lot of performance on the table for common operations. Custom Triton kernels can close that gap. For RMSNorm, the gap was 8x.</p> <p>But unless you have specific needs, use the battle-tested libraries. The wins from custom kernels often don’t justify the maintenance burden. Write your own to learn, but deploy with FlashAttention and vLLM.</p> <hr> <p><em>This is Part 2 of my LLM inference series. <a href="/blog/2025/making-llm-faster/">Part 1 covered speculative decoding</a> - a different angle on the same memory-bound problem. Questions or feedback? Open an issue on the repo or reach out.</em></p> </div> </article> <div class="citation-section"> <div class="citation-header"> <h3>Citation</h3> <p class="citation-intro">If you found this article useful, please cite it using one of the formats below:</p> </div> <div class="citation-content"> <div class="citation-format"> <h4>APA Format</h4> <div class="citation-text"> <p>Mitra, Subhadip. (2025, June). <em>From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference</em>. Retrieved from https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/</p> </div> </div> <div class="citation-format"> <h4>BibTeX Entry</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">mitra2025from-11-to-88-peak-bandwidth-writing-custom-triton-kernels-for-llm-inference</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Mitra, Subhadip}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Jun}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> </div> <style>.post-footer-featured{margin-top:6rem;padding-top:0}.newsletter-featured{background:var(--global-card-bg-color,#fff);border:2px solid var(--global-theme-color,#b509ac);border-radius:24px;padding:3rem;margin-bottom:3rem}.showcase-label-newsletter{display:inline-flex;align-items:center;gap:.5rem;font-size:.875rem;letter-spacing:.15em;text-transform:uppercase;color:var(--global-theme-color,#b509ac);font-weight:700;margin-bottom:1rem}.newsletter-featured h2{font-size:2rem;font-weight:900;margin:0 0 1rem 0;letter-spacing:-0.02em;line-height:1.1;color:var(--global-text-color,#000)}.newsletter-featured p{font-size:1.125rem;color:var(--global-text-color-light,#666);margin-bottom:1.5rem;line-height:1.6}.newsletter-form-featured{display:flex;gap:.75rem;margin-bottom:1rem}.newsletter-form-featured input{flex:1;padding:.875rem 1.25rem;background:var(--global-bg-color,#fff);color:var(--global-text-color,#000);border:1px solid var(--global-divider-color,#e5e5e5);border-radius:12px;font-size:1rem;transition:all .3s}.newsletter-form-featured input:focus{outline:0;border-color:var(--global-theme-color,#b509ac);box-shadow:0 0 0 3px rgba(181,9,172,0.1)}.newsletter-form-featured button{padding:.875rem 2rem;background:var(--global-theme-color,#b509ac);color:white;border:0;border-radius:12px;font-weight:700;font-size:1rem;cursor:pointer;transition:all .3s;white-space:nowrap}.newsletter-form-featured button:hover{transform:translateY(-4px);box-shadow:0 12px 40px rgba(181,9,172,0.3)}.newsletter-links{display:flex;gap:1rem;flex-wrap:wrap;align-items:center;font-size:.875rem;color:var(--global-text-color-light,#666)}.newsletter-links a{color:var(--global-theme-color,#b509ac);text-decoration:none;font-weight:600;transition:opacity .3s}.newsletter-links a:hover{opacity:.7}.share-section-featured{text-align:center;padding:2rem 0 3rem 0;border-bottom:1px solid var(--global-divider-color,#e5e5e5)}.share-section-featured .share-label{font-size:.875rem;text-transform:uppercase;letter-spacing:.1em;color:var(--global-text-color-light,#666);font-weight:600;margin-bottom:1.25rem}.share-buttons-featured{display:flex;gap:1rem;justify-content:center;align-items:center}.share-link-featured{display:inline-flex;align-items:center;gap:.5rem;font-weight:600;color:var(--global-text-color,#000);text-decoration:none;font-size:.875rem;padding:.5rem 1rem;background:var(--global-code-bg-color,#f8f9fa);border:1px solid var(--global-divider-color,#e5e5e5);border-radius:8px;transition:all .2s ease;white-space:nowrap;cursor:pointer}.share-link-featured:hover{background:var(--global-theme-color,#b509ac);border-color:var(--global-theme-color,#b509ac);color:white;transform:translateY(-2px)}.share-link-featured:hover svg{fill:white}.share-link-featured svg{width:16px;height:16px;flex-shrink:0;transition:fill .2s ease}.related-posts-section{margin-top:3rem;padding-top:0}.related-posts-section h3{font-size:1.75rem;font-weight:800;margin-bottom:2rem;color:var(--global-text-color,#000);text-align:center}@media(max-width:768px){.newsletter-featured{padding:2rem}.newsletter-featured h2{font-size:1.5rem}.newsletter-form-featured{flex-direction:column}.share-buttons-featured{flex-wrap:wrap;gap:.5rem}}</style> <div class="post-footer-featured"> <div class="newsletter-featured"> <div class="showcase-label-newsletter"> <svg width="16" height="16" viewbox="0 0 24 24" fill="currentColor"> <path d="M12 2l3.09 6.26L22 9.27l-5 4.87 1.18 6.88L12 17.77l-6.18 3.25L7 14.14 2 9.27l6.91-1.01L12 2z"></path> </svg> Strategic Insights </div> <h2>Get More Like This</h2> <p> Join technical leaders from Google, Amazon, and Fortune 500s. Get strategic insights on Data, AI, and Cloud transformation delivered to your inbox. </p> <form class="newsletter-form-featured" action="https://app.loops.so/api/newsletter-form/cm614n2d604nlfy1lfo7vgmo5" method="POST"> <input type="email" name="email" placeholder="your@email.com" required> <button type="submit">Subscribe</button> </form> <div class="newsletter-links"> <span>Free insights • No spam • Unsubscribe anytime</span> <span>•</span> <a href="/archive/">Browse the archive →</a> </div> </div> <div class="share-section-featured"> <div class="share-label">Share This Article</div> <div class="share-buttons-featured"> <a href="https://twitter.com/intent/tweet?text=From%2011%25%20to%2088%25%20Peak%20Bandwidth:%20Writing%20Custom%20Triton%20Kernels%20for%20LLM%20Inference&amp;url=https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/&amp;via=bassrehab" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on X"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.6823 10.6218L20.2391 3H18.6854L12.9921 9.61788L8.44486 3H3.2002L10.0765 13.0074L3.2002 21H4.75404L10.7663 14.0113L15.5685 21H20.8131L13.6819 10.6218H13.6823ZM11.5541 13.0956L10.8574 12.0991L5.31391 4.16971H7.70053L12.1742 10.5689L12.8709 11.5655L18.6861 19.8835H16.2995L11.5541 13.096V13.0956Z"></path> </svg> <span>X</span> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on LinkedIn"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path> </svg> <span>LinkedIn</span> </a> <a href="https://www.reddit.com/submit?url=https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/&amp;title=From%2011%25%20to%2088%25%20Peak%20Bandwidth:%20Writing%20Custom%20Triton%20Kernels%20for%20LLM%20Inference" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on Reddit"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M12 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 0 1-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 0 1 .042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 0 1 4.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 0 1 .14-.197.35.35 0 0 1 .238-.042l2.906.617a1.214 1.214 0 0 1 1.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 0 0-.231.094.33.33 0 0 0 0 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 0 0 .029-.463.33.33 0 0 0-.464 0c-.547.533-1.684.73-2.512.73-.828 0-1.979-.196-2.512-.73a.326.326 0 0 0-.232-.095z"></path> </svg> <span>Reddit</span> </a> <a href="https://news.ycombinator.com/submitlink?u=https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/&amp;t=From%2011%25%20to%2088%25%20Peak%20Bandwidth:%20Writing%20Custom%20Triton%20Kernels%20for%20LLM%20Inference" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on Hacker News"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M0 0v24h24V0H0zm12.3 12.6l-3.6-7.2H10l2.4 5.1 2.4-5.1h1.3l-3.6 7.2v4.8h-1.2v-4.8z"></path> </svg> <span>HN</span> </a> <a href="mailto:?subject=From%2011%25%20to%2088%25%20Peak%20Bandwidth:%20Writing%20Custom%20Triton%20Kernels%20for%20LLM%20Inference&amp;body=Check%20out%20this%20article:%20https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/" class="share-link-featured" title="Share via Email"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"></path> </svg> <span>Email</span> </a> <button onclick="copyLinkFeatured('https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/')" class="share-link-featured" title="Copy link to clipboard"> <svg class="copy-icon" viewbox="0 0 24 24" fill="currentColor"> <path d="M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z"></path> </svg> <svg class="check-icon" style="display: none;" viewbox="0 0 24 24" fill="currentColor"> <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path> </svg> <span class="copy-text">Copy</span> <span class="copied-text" style="display: none;">Copied!</span> </button> </div> </div> <div class="related-posts-section"> <h3>Continue Reading</h3> </div> </div> <script>
function copyLinkFeatured(url) {
  navigator.clipboard.writeText(url).then(() => {
    const btn = event.target.closest('button');
    const copyText = btn.querySelector('.copy-text');
    const copiedText = btn.querySelector('.copied-text');
    const copyIcon = btn.querySelector('.copy-icon');
    const checkIcon = btn.querySelector('.check-icon');

    copyText.style.display = 'none';
    copiedText.style.display = 'inline';
    copyIcon.style.display = 'none';
    checkIcon.style.display = 'inline';

    setTimeout(() => {
      copyText.style.display = 'inline';
      copiedText.style.display = 'none';
      copyIcon.style.display = 'inline';
      checkIcon.style.display = 'none';
    }, 2000);
  }).catch(err => {
    console.error('Failed to copy:', err);
    alert('Could not copy link. Please copy manually: ' + url);
  });
}

// Newsletter form handler
(function() {
  const form = document.querySelector('.newsletter-form-featured');
  if (!form) return;

  form.addEventListener('submit', function(event) {
    event.preventDefault();

    const emailInput = form.querySelector('input[name="email"]');
    const submitBtn = form.querySelector('button[type="submit"]');
    const email = emailInput.value;

    // Disable submit button and show loading state
    submitBtn.disabled = true;
    const originalText = submitBtn.textContent;
    submitBtn.textContent = 'Subscribing...';

    // Submit to Loops.so
    const formBody = 'userGroup=&email=' + encodeURIComponent(email);
    fetch(form.action, {
      method: 'POST',
      body: formBody,
      headers: {
        'Content-Type': 'application/x-www-form-urlencoded',
      },
    })
    .then(res => res.json().then(data => ({ ok: res.ok, data })))
    .then(({ ok, data }) => {
      if (ok) {
        // Success - show confirmation message
        submitBtn.textContent = '✓ Subscribed!';
        submitBtn.style.background = '#22c55e';
        emailInput.value = '';

        // Reset after 3 seconds
        setTimeout(() => {
          submitBtn.textContent = originalText;
          submitBtn.style.background = '';
          submitBtn.disabled = false;
        }, 3000);
      } else {
        // Error from API
        throw new Error(data.message || 'Subscription failed');
      }
    })
    .catch(error => {
      // Show error
      submitBtn.textContent = '✗ Failed';
      submitBtn.style.background = '#ef4444';
      console.error('Newsletter subscription error:', error);

      // Reset after 3 seconds
      setTimeout(() => {
        submitBtn.textContent = originalText;
        submitBtn.style.background = '';
        submitBtn.disabled = false;
      }, 3000);
    });
  });
})();
</script> <style>.related-posts-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:1.5rem}.related-post-card{background:var(--global-card-bg-color,#fff);border:1px solid var(--global-divider-color,#e5e5e5);border-radius:16px;padding:2rem;transition:all .3s cubic-bezier(0.4,0,0.2,1);position:relative;display:flex;flex-direction:column}.related-post-card:hover{transform:translateY(-4px);box-shadow:0 12px 32px rgba(0,0,0,0.1);border-color:var(--global-theme-color,#b509ac)}.related-post-title{font-size:1.25rem;font-weight:700;line-height:1.3;margin:0 0 .75rem 0;color:var(--global-text-color,#000)}.related-post-title a{color:inherit;text-decoration:none;transition:color .3s}.related-post-title a:hover{color:var(--global-theme-color,#b509ac)}.related-post-meta{font-size:.875rem;color:var(--global-text-color-light,#666);margin-bottom:.75rem}.related-post-excerpt{font-size:.95rem;line-height:1.6;color:var(--global-text-color-light,#666);margin-bottom:1rem;flex-grow:1}.related-post-link{display:inline-flex;align-items:center;gap:.5rem;font-weight:700;color:var(--global-theme-color,#b509ac);text-decoration:none;font-size:.95rem;transition:gap .3s;margin-top:auto}.related-post-link:hover{gap:1rem}.related-post-link svg{width:16px;height:16px;transition:transform .3s}.related-post-link:hover svg{transform:translateX(4px)}.external-link-icon{width:14px;height:14px;opacity:.6;margin-left:.25rem}.no-related-posts{text-align:center;padding:3rem;color:var(--global-text-color-light,#666);font-style:italic}@media(max-width:768px){.related-posts-grid{grid-template-columns:1fr;gap:1rem}.related-post-card{padding:1.5rem}.related-post-title{font-size:1.125rem}}</style> <div class="related-posts-container"> <div class="related-posts-grid"> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2025/detecting-ai-sandbagging/">I Trained Probes to Catch AI Models Sandbagging</a> </h4> <div class="related-post-meta"> December 20, 2025 • sandbagging </div> <p class="related-post-excerpt">First empirical demonstration of activation-level sandbagging detection. Linear probes achieve 90-96% accuracy across Mistral, Gemma, and Qwen models. Key finding - sandbagging representations are model-specific,...</p> <a href="/blog/2025/detecting-ai-sandbagging/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2025/steering-vectors-agents/">Why Steering Vectors Beat Prompting (And When They Don't)</a> </h4> <div class="related-post-meta"> December 18, 2025 • machine-learning </div> <p class="related-post-excerpt">I tested activation steering on 4 agent behaviors across 3 models. The results surprised me.</p> <a href="/blog/2025/steering-vectors-agents/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2025/building-spark-llm-eval/">Why I Built a Spark-Native LLM Evaluation Framework (And What I Learned)</a> </h4> <div class="related-post-meta"> December 15, 2025 • llm-evaluation </div> <p class="related-post-excerpt">A deep dive into building distributed LLM evaluation infrastructure that actually scales - architectural decisions, trade-offs, and lessons learned.</p> <a href="/blog/2025/building-spark-llm-eval/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2025/mcp-maturity-model/">The MCP Maturity Model: Evaluating Your Multi-Agent Context Strategy</a> </h4> <div class="related-post-meta"> November 19, 2025 • MCP </div> <p class="related-post-excerpt">A practical framework for evaluating your multi-agent context management strategy. From ad-hoc string concatenation to self-evolving context systems - where does your architecture stand?</p> <a href="/blog/2025/mcp-maturity-model/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> </div> </div> <br> <br> <div class="hyvor-talk-container"> <style>.hyvor-talk-container{margin-top:4rem;padding-top:3rem;border-top:2px solid var(--global-divider-color,#e5e5e5)}.hyvor-talk-header{margin-bottom:2rem;text-align:center}.hyvor-talk-header h3{font-size:1.75rem;font-weight:800;margin-bottom:.5rem;color:var(--global-text-color,#000)}.hyvor-talk-header p{font-size:1rem;color:var(--global-text-color-light,#666);margin:0}</style> <div class="hyvor-talk-header"> <h3>Join the Discussion</h3> <p>Share your thoughts, ask questions, or provide feedback on this article</p> </div> <div id="hyvor-talk-view"></div> <script type="application/javascript">
    var HYVOR_TALK_WEBSITE = 14339;
    var HYVOR_TALK_CONFIG = {
      url: 'https://subhadipmitra.com/blog/2025/triton-kernels-llm-inference/',
      id: '/blog/2025/triton-kernels-llm-inference/',
      
      title: 'From 11% to 88% Peak Bandwidth: Writing Custom Triton Kernels for LLM Inference',
      
      
      palette: {
        accent: '#b509ac',
        accentText: '#ffffff',
        footerHeader: 'var(--global-text-color)',
        footerHeaderText: 'var(--global-text-color-light)',
        box: 'var(--global-card-bg-color)',
        boxText: 'var(--global-text-color)',
        boxLightText: 'var(--global-text-color-light)',
        backgroundText: 'var(--global-text-color)'
      }
      
    };
  </script> <script async type="application/javascript" src="//talk.hyvor.com/web-api/embed.js"></script> </div> </div> </div> </div> </div> <footer class="modern-footer" role="contentinfo"> <div class="footer-container"> <div class="footer-grid"> <div class="footer-brand"> <h3>Subhadip Mitra</h3> <p> Technical Leader, Inventor, and Researcher building the future of Data &amp; Applied AI. <br>Leading Google Cloud's D&amp;A practice across Southeast Asia. </p> <br> <p><a href="mailto:contact@subhadipmitra.com">contact@subhadipmitra.com</a></p> </div> <div class="footer-section"> <h4>Explore</h4> <ul class="footer-links"> <li><a href="/">About</a></li> <li><a href="/now/">What I'm Doing Now</a></li> <li><a href="/blog/">Blog</a></li> <li><a href="/publications/">Publications</a></li> <li><a href="/repositories/">Repositories</a></li> </ul> </div> <div class="footer-section"> <h4>Connect</h4> <ul class="footer-links"> <li><a href="mailto:contact@subhadipmitra.com">Email</a></li> <li><a href="https://calendly.com/contact-x9nm/30min" target="_blank" rel="external nofollow noopener">Schedule a Call</a></li> <li><a href="https://linkedin.com/in/subhadip-mitra" target="_blank" rel="external nofollow noopener">LinkedIn</a></li> <li><a href="https://github.com/bassrehab" target="_blank" rel="external nofollow noopener">GitHub</a></li> </ul> </div> </div> <div class="footer-bottom"> <div class="footer-copyright"> © 2025 Subhadip Mitra. Some Rights Reserved. Last updated: December 23, 2025. <span class="footer-utility-links"> <a href="/sitemap.xml" title="Sitemap">Sitemap</a> · <a href="/feed.xml" title="RSS Feed">RSS</a> · <a href="/llms.txt" title="AI Agent Information">LLMs</a> </span> </div> <div class="footer-social"> <a href="https://linkedin.com/in/subhadip-mitra" target="_blank" aria-label="LinkedIn" rel="external nofollow noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://github.com/bassrehab" target="_blank" aria-label="GitHub" rel="external nofollow noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/bassrehab" target="_blank" aria-label="Twitter" rel="external nofollow noopener"> <i class="fab fa-twitter"></i> </a> <a href="mailto:contact@subhadipmitra.com" aria-label="Email"> <i class="fas fa-envelope"></i> </a> </div> </div> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-TW7YQ5XPC6"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-TW7YQ5XPC6');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <script defer src="/assets/js/newsletter.js?c3d0931971ee96e9df74ba70526c3130"></script> </body> </html>