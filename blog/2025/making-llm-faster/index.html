<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="R3nPzqP9nO2aTgysGUiU3t-pw3wi1xuUuKVaMrimuck"> <meta name="msvalidate.01" content="04fec5e0d64b1c9bc788e98f81ee2a78"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Making LLMs Faster: My Deep Dive into Speculative Decoding | Subhadip Mitra </title> <meta name="author" content="Subhadip Mitra"> <meta name="description" content="A deep dive into implementing speculative decoding from scratch, with benchmarks on GPT-2 and extensions to diffusion models."> <meta name="keywords" content="subhadip-mitra,subhadip,google,google-cloud,data-&amp;-analytics,ai-innovations,enterprise-technology-leadership,digital-transformation,machine-learning-models,cloud-technologies,quantum-computing,technology-consulting,singapore,researcher,llm,genai"> <meta property="og:site_name" content="Subhadip Mitra"> <meta property="og:type" content="article"> <meta property="og:title" content="Subhadip Mitra | Making LLMs Faster: My Deep Dive into Speculative Decoding"> <meta property="og:url" content="https://subhadipmitra.com/blog/2025/making-llm-faster/"> <meta property="og:description" content="A deep dive into implementing speculative decoding from scratch, with benchmarks on GPT-2 and extensions to diffusion models."> <meta property="og:image" content="https://subhadipmitra.com/assets/img/og/making-llm-faster.png"> <meta property="og:image:width" content="1200"> <meta property="og:image:height" content="630"> <meta property="og:locale" content="en"> <meta property="og:logo" content="https://subhadipmitra.com/assets/img/prof_pic.jpg"> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="Making LLMs Faster: My Deep Dive into Speculative Decoding"> <meta name="twitter:description" content="A deep dive into implementing speculative decoding from scratch, with benchmarks on GPT-2 and extensions to diffusion models."> <meta name="twitter:image" content="https://subhadipmitra.com/assets/img/og/making-llm-faster.png"> <meta property="article:published_time" content="2025-03-20T01:02:56+00:00"> <meta property="article:author" content="Subhadip Mitra"> <meta property="article:tag" content="machine-learning"> <meta property="article:tag" content="llm"> <meta property="article:tag" content="inference"> <meta property="article:tag" content="optimization"> <meta property="article:section" content="AI Infrastructure"> <meta property="article:section" content="Research"> <script type="application/ld+json">
      {
          "@context": "https://schema.org",
          "@type": "BlogPosting",
          "mainEntityOfPage": {
              "@type": "WebPage",
              "@id": "https://subhadipmitra.com/blog/2025/making-llm-faster/"
          },
          "headline": "Making LLMs Faster: My Deep Dive into Speculative Decoding",
          "description": "A deep dive into implementing speculative decoding from scratch, with benchmarks on GPT-2 and extensions to diffusion models.",
          "url": "https://subhadipmitra.com/blog/2025/making-llm-faster/",
          "datePublished": "2025-03-20T01:02:56+00:00",
          
          "dateModified": "2025-03-20T01:02:56+00:00",
          
          "wordCount": 2298,
          "timeRequired": "PT12M",
          
          "image": "https://subhadipmitra.com/assets/img/blog/roofline.png",
          
          
          "keywords": "machine-learning, llm, inference, optimization",
          
          
          "articleSection": "AI Infrastructure",
          
          "author": {
              "@type": "Person",
              "name": "Subhadip Mitra",
              "url": "https://subhadipmitra.com",
              "jobTitle": "Data and Analytics Manager, Site Lead Southeast Asia",
              "worksFor": {
                  "@type": "Organization",
                  "name": "Google Cloud"
              },
              "sameAs": [
                  "https://linkedin.com/in/subhadip-mitra",
                  "https://github.com/bassrehab",
                  "https://twitter.com/bassrehab"
              ]
          },
          "publisher": {
              "@type": "Organization",
              "name": "Subhadip Mitra",
              "url": "https://subhadipmitra.com",
              "logo": {
                  "@type": "ImageObject",
                  "url": "https://subhadipmitra.com/assets/img/prof_pic.jpg"
              }
          },
          "isPartOf": {
              "@type": "Blog",
              "name": "binary breakthroughs",
              "url": "https://subhadipmitra.com/blog/"
          }
          
      }
    </script> <script type="application/ld+json">
      {
          "@context": "https://schema.org",
          "@type": "BreadcrumbList",
          "itemListElement": [
              {
                  "@type": "ListItem",
                  "position": 1,
                  "name": "Home",
                  "item": "https://subhadipmitra.com"
              },
              {
                  "@type": "ListItem",
                  "position": 2,
                  "name": "Blog",
                  "item": "https://subhadipmitra.com/blog/"
              },
              
              {
                  "@type": "ListItem",
                  "position": 3,
                  "name": "Ai infrastructure",
                  "item": "https://subhadipmitra.com/blog/category/ai-infrastructure"
              },
              {
                  "@type": "ListItem",
                  "position": 4,
                  "name": "Making LLMs Faster: My Deep Dive into Speculative Decoding",
                  "item": "https://subhadipmitra.com/blog/2025/making-llm-faster/"
              }
              
          ]
      }
    </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&amp;family=Inter+Tight:ital,wght@0,100..900;1,100..900&amp;family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?058310854f7b3c920f894a1d42c3c47b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://subhadipmitra.com/blog/2025/making-llm-faster/"> <script src="/assets/js/theme.js?33a804e644f2507dbddc853404eccc7a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-md fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Subhadip</span> Mitra </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/now/">now </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/bets/">bets </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/repositories/">repositories</a> <a class="dropdown-item " href="/reading-list/">reading list</a> <a class="dropdown-item " href="/contact/">contact</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://artemis.subhadipmitra.com" rel="external nofollow noopener" target="_blank">ARTEMIS</a> <a class="dropdown-item " href="https://upir.subhadipmitra.com" rel="external nofollow noopener" target="_blank">UPIR</a> <a class="dropdown-item " href="https://ai-metacognition-toolkit.subhadipmitra.com/" rel="external nofollow noopener" target="_blank">AI Metacognition</a> <a class="dropdown-item " href="https://docs.smppgateway.io" rel="external nofollow noopener" target="_blank">SMPP Gateway</a> <a class="dropdown-item " href="https://iso8583.subhadipmitra.com" rel="external nofollow noopener" target="_blank">ISO8583</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://symmetry.subhadipmitra.com/" rel="external nofollow noopener" target="_blank">symmetry</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <div id="toc-wrapper" class="toc-wrapper sticky-top"> <div class="toc-header"> <span class="toc-title">Contents</span> <button class="toc-toggle" aria-label="Toggle table of contents" title="Hide contents"> <i class="fas fa-chevron-up"></i> </button> </div> <nav id="toc-sidebar" class="toc-nav"></nav> </div> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Making LLMs Faster: My Deep Dive into Speculative Decoding</h1> <p class="post-meta"> Created on March 20, 2025 by Subhadip Mitra </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/inference"> <i class="fa-solid fa-hashtag fa-sm"></i> inference</a>   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>   ·   <a href="/blog/category/ai-infrastructure"> <i class="fa-solid fa-tag fa-sm"></i> AI Infrastructure</a>   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a> </p> </header> <div class="audio-reader-container"> <div class="audio-reader-compact"> <button class="audio-trigger" id="audio-trigger" aria-label="Open audio player"> <i class="fas fa-headphones"></i> <span>Listen to article</span> <i class="fas fa-chevron-down audio-trigger-icon"></i> </button> <div class="audio-player-expanded" id="audio-player-expanded"> <div class="audio-player-body"> <div class="audio-info-bar"> <i class="fas fa-info-circle"></i> <span>Browser text-to-speech • Quality varies by device</span> </div> <div class="audio-controls-row"> <button class="audio-btn audio-btn-play" id="tts-play" aria-label="Play"> <i class="fas fa-play"></i> <span id="play-text">Play</span> </button> <button class="audio-btn audio-btn-pause" id="tts-pause" style="display: none" aria-label="Pause"> <i class="fas fa-pause"></i> <span>Pause</span> </button> <button class="audio-btn audio-btn-stop" id="tts-stop" aria-label="Stop"> <i class="fas fa-stop"></i> </button> <div class="audio-speed-control"> <label for="tts-rate" class="audio-speed-label"> <i class="fas fa-gauge-high"></i> <span id="tts-rate-value">1.0x</span> </label> <input id="tts-rate" type="range" min="0.5" max="2" step="0.1" value="1" class="audio-speed-slider"> </div> </div> <div class="audio-voice-row"> <label for="tts-voice" class="audio-voice-label"> <i class="fas fa-microphone"></i> <span>Voice:</span> </label> <select id="tts-voice" class="audio-voice-select"> <option value="">Loading voices...</option> </select> </div> <div class="audio-progress-section"> <div class="audio-progress-info"> <span><i class="fas fa-circle-notch"></i> Progress</span> <span id="tts-progress">0%</span> </div> <div class="progress" style="height: 4px"> <div class="progress-bar" id="tts-progress-bar" role="progressbar" style="width: 0%" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100"></div> </div> </div> </div> </div> </div> </div> <style>.audio-reader-container{margin:1.5rem 0;font-family:var(--global-font-family,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,sans-serif)}.audio-reader-compact{position:relative}.audio-trigger{display:inline-flex;align-items:center;gap:.5rem;padding:.5rem 1rem;background:var(--global-code-bg-color,#f8f9fa);border:1px solid var(--global-divider-color,#dee2e6);border-radius:.375rem;color:var(--global-text-color,#212529);font-size:.875rem;font-weight:500;cursor:pointer;transition:all .2s ease;font-family:inherit}.audio-trigger:hover{background:var(--global-bg-color,#fff);border-color:var(--global-theme-color,#0d6efd);color:var(--global-theme-color,#0d6efd);box-shadow:0 .125rem .25rem rgba(0,0,0,0.075)}.audio-trigger i:first-child{font-size:1rem;color:var(--global-theme-color,#0d6efd)}.audio-trigger-icon{font-size:.75rem;margin-left:.25rem;transition:transform .3s ease;color:var(--global-text-color-light,#6c757d)}.audio-trigger.active .audio-trigger-icon{transform:rotate(180deg)}.audio-trigger:focus{outline:2px solid var(--global-theme-color,#0d6efd);outline-offset:2px}.audio-player-expanded{display:none;margin-top:.75rem;background:var(--global-bg-color,#fff);border:1px solid var(--global-divider-color,#dee2e6);border-radius:.5rem;box-shadow:0 .25rem .75rem rgba(0,0,0,0.05);animation:slideDown .3s ease}.audio-player-expanded.show{display:block}@keyframes slideDown{from{opacity:0;transform:translateY(-0.5rem)}to{opacity:1;transform:translateY(0)}}.audio-player-body{padding:1rem}.audio-info-bar{display:flex;align-items:center;gap:.5rem;padding:.625rem .75rem;background:var(--global-code-bg-color,#f8f9fa);border-left:2px solid var(--global-theme-color,#0d6efd);border-radius:.25rem;font-size:.8125rem;color:var(--global-text-color-light,#6c757d);margin-bottom:1rem}.audio-info-bar i{color:var(--global-theme-color,#0d6efd);font-size:.875rem}.audio-controls-row{display:flex;align-items:center;gap:.5rem;margin-bottom:.875rem;flex-wrap:wrap}.audio-btn{display:inline-flex;align-items:center;gap:.375rem;padding:.375rem .875rem;border:1px solid transparent;border-radius:.375rem;font-size:.875rem;font-weight:500;cursor:pointer;transition:all .15s ease;font-family:inherit;color:#fff}.audio-btn i{font-size:.8125rem}.audio-btn span{color:#fff}.audio-btn:hover:not(:disabled){transform:translateY(-1px);box-shadow:0 .125rem .375rem rgba(0,0,0,0.15)}.audio-btn:active:not(:disabled){transform:translateY(0)}.audio-btn:disabled{opacity:.6;cursor:not-allowed}.audio-btn-play{background:var(--global-theme-color,#0d6efd);border-color:var(--global-theme-color,#0d6efd)}.audio-btn-play:hover:not(:disabled){background:color-mix(in srgb,var(--global-theme-color,#0d6efd) 85%,black);border-color:color-mix(in srgb,var(--global-theme-color,#0d6efd) 80%,black)}.audio-btn-play.playing{background:#198754;border-color:#198754}.audio-btn-pause{background:#6c757d;border-color:#6c757d}.audio-btn-pause:hover:not(:disabled){background:#5c636a;border-color:#565e64}.audio-btn-stop{background:transparent;color:#dc3545;border-color:#dc3545;padding:.375rem .625rem}.audio-btn-stop:hover:not(:disabled){background:#dc3545;color:#fff;border-color:#dc3545}.audio-speed-control{display:flex;align-items:center;gap:.5rem;margin-left:auto}.audio-speed-label{display:flex;align-items:center;gap:.375rem;font-size:.8125rem;font-weight:500;color:var(--global-text-color,#212529);margin:0;white-space:nowrap}.audio-speed-label i{color:var(--global-theme-color,#0d6efd);font-size:.875rem}.audio-speed-slider{width:80px;height:4px;-webkit-appearance:none;appearance:none;background:var(--global-divider-color,#dee2e6);border-radius:2px;outline:0}.audio-speed-slider::-webkit-slider-thumb{-webkit-appearance:none;appearance:none;width:14px;height:14px;border-radius:50%;background:var(--global-theme-color,#0d6efd);cursor:pointer;transition:transform .15s ease}.audio-speed-slider::-webkit-slider-thumb:hover{transform:scale(1.2)}.audio-speed-slider::-moz-range-thumb{width:14px;height:14px;border-radius:50%;background:var(--global-theme-color,#0d6efd);border:0;cursor:pointer;transition:transform .15s ease}.audio-speed-slider::-moz-range-thumb:hover{transform:scale(1.2)}.audio-voice-row{display:flex;align-items:center;gap:.75rem;margin-bottom:.875rem}.audio-voice-label{display:flex;align-items:center;gap:.375rem;font-size:.8125rem;font-weight:500;color:var(--global-text-color,#212529);margin:0;white-space:nowrap}.audio-voice-label i{color:var(--global-theme-color,#0d6efd);font-size:.875rem}.audio-voice-select{flex:1;padding:.375rem .75rem;border:1px solid var(--global-divider-color,#dee2e6);border-radius:.375rem;font-size:.8125rem;background:var(--global-bg-color,#fff);color:var(--global-text-color,#212529);cursor:pointer;transition:border-color .15s ease}.audio-voice-select:hover{border-color:var(--global-theme-color,#0d6efd)}.audio-voice-select:focus{outline:0;border-color:var(--global-theme-color,#0d6efd);box-shadow:0 0 0 .2rem rgba(13,110,253,0.25)}.audio-progress-section{margin-top:.75rem;padding-top:.75rem;border-top:1px solid var(--global-divider-color,#e9ecef)}
.audio-progress-info{display:flex;justify-content:space-between;align-items:center;font-size:.8125rem;color:var(--global-text-color-light,#6c757d);margin-bottom:.5rem}.audio-progress-info i{margin-right:.25rem;color:var(--global-theme-color,#0d6efd);font-size:.75rem}.progress{border-radius:2px;overflow:hidden;background:var(--global-divider-color,#e9ecef)}.progress-bar{background:var(--global-theme-color,#0d6efd);transition:width .6s ease}[data-theme="dark"] .audio-trigger,body.dark-mode .audio-trigger{background:rgba(255,255,255,0.05);border-color:var(--global-divider-color,#454545);color:var(--global-text-color,#e9ecef)}[data-theme="dark"] .audio-trigger:hover,body.dark-mode .audio-trigger:hover{background:rgba(255,255,255,0.08)}[data-theme="dark"] .audio-player-expanded,body.dark-mode .audio-player-expanded{background:var(--global-bg-color,#1e1e1e);border-color:var(--global-divider-color,#454545)}[data-theme="dark"] .audio-info-bar,body.dark-mode .audio-info-bar{background:rgba(255,255,255,0.03)}[data-theme="dark"] .audio-voice-select,body.dark-mode .audio-voice-select{background:var(--global-bg-color,#1e1e1e);border-color:var(--global-divider-color,#454545);color:var(--global-text-color,#e9ecef)}[data-theme="dark"] .progress,body.dark-mode .progress{background:rgba(255,255,255,0.1)}@media(max-width:576px){.audio-trigger span{display:none}.audio-controls-row{flex-wrap:wrap}.audio-btn span{display:none}.audio-btn{padding:.5rem .75rem}.audio-btn i{font-size:1rem}.audio-speed-control{width:100%;margin-left:0;margin-top:.5rem}.audio-speed-slider{flex:1}.audio-voice-row{flex-direction:column;align-items:stretch}}@media print{.audio-reader-container{display:none!important}}@media(prefers-reduced-motion:reduce){.audio-trigger,.audio-btn,.progress-bar,.audio-player-expanded{transition:none;animation:none}}</style> <script>
  (function () {
    "use strict";

    if (document.readyState === "loading") {
      document.addEventListener("DOMContentLoaded", init);
    } else {
      init();
    }

    function init() {
      if (!("speechSynthesis" in window)) {
        document.querySelector(".audio-reader-container").innerHTML = `
        <div class="audio-reader-compact">
          <div class="alert alert-warning d-flex align-items-center" role="alert" style="font-size: 0.875rem;">
            <i class="fas fa-exclamation-triangle me-2"></i>
            <div>Text-to-speech not supported in this browser.</div>
          </div>
        </div>
      `;
        return;
      }

      const synth = window.speechSynthesis;
      let utterance = null;
      let isPaused = false;

      const triggerBtn = document.getElementById("audio-trigger");
      const expandedPlayer = document.getElementById("audio-player-expanded");
      const playBtn = document.getElementById("tts-play");
      const pauseBtn = document.getElementById("tts-pause");
      const stopBtn = document.getElementById("tts-stop");
      const voiceSelect = document.getElementById("tts-voice");
      const rateSlider = document.getElementById("tts-rate");
      const rateValue = document.getElementById("tts-rate-value");
      const progressText = document.getElementById("tts-progress");
      const progressBar = document.getElementById("tts-progress-bar");
      const playText = document.getElementById("play-text");

      const possibleSelectors = [
        ".post-content",
        "article .post-content",
        ".post .post-content",
        "article.post",
        ".post",
        "article",
        "main article",
        "main .container",
        "main",
        ".l-main",
        ".content",
      ];

      let articleElement = null;
      for (const selector of possibleSelectors) {
        articleElement = document.querySelector(selector);
        if (articleElement) break;
      }

      if (!articleElement) {
        console.warn("Audio Reader: Could not find article content");
        return;
      }

      const clonedContent = articleElement.cloneNode(true);
      const elementsToRemove = [
        "pre",
        "code",
        "script",
        "style",
        "nav",
        ".audio-reader-container",
        ".citation",
        ".giscus",
        ".utterances",
        ".navigation",
        ".pagination",
        ".social",
        ".share",
        "header",
        "footer",
        ".header",
        ".footer",
        ".sidebar",
      ];

      elementsToRemove.forEach((selector) => {
        clonedContent.querySelectorAll(selector).forEach((el) => el.remove());
      });

      const articleText = clonedContent.innerText
        .replace(/\s+/g, " ")
        .replace(/\n{3,}/g, "\n\n")
        .trim();

      if (!articleText || articleText.length < 100) {
        console.warn("Audio Reader: Article too short");
        return;
      }

      // Toggle player
      triggerBtn.addEventListener("click", function () {
        expandedPlayer.classList.toggle("show");
        this.classList.toggle("active");
        localStorage.setItem("audioPlayerExpanded", expandedPlayer.classList.contains("show"));
      });

      // Load voices
      function loadVoices() {
        const voices = synth.getVoices();
        if (voices.length === 0) {
          setTimeout(loadVoices, 100);
          return;
        }

        voiceSelect.innerHTML = '<option value="">Default Voice</option>';

        const englishVoices = voices.filter((v) => v.lang.startsWith("en"));
        const preferredVoices = englishVoices.filter(
          (v) =>
            v.name.includes("Google") ||
            v.name.includes("Microsoft") ||
            v.name.includes("Premium") ||
            v.name.includes("Enhanced") ||
            v.name.includes("Natural")
        );
        const standardEnglishVoices = englishVoices.filter((v) => !preferredVoices.includes(v));
        const otherVoices = voices.filter((v) => !v.lang.startsWith("en"));

        preferredVoices.forEach((voice) => {
          const option = document.createElement("option");
          option.value = voices.indexOf(voice);
          option.textContent = `${voice.name} (${voice.lang})`;
          voiceSelect.appendChild(option);
        });

        if (standardEnglishVoices.length > 0) {
          const optgroup = document.createElement("optgroup");
          optgroup.label = "Other English";
          standardEnglishVoices.forEach((voice) => {
            const option = document.createElement("option");
            option.value = voices.indexOf(voice);
            option.textContent = `${voice.name} (${voice.lang})`;
            optgroup.appendChild(option);
          });
          voiceSelect.appendChild(optgroup);
        }

        if (otherVoices.length > 0) {
          const optgroup = document.createElement("optgroup");
          optgroup.label = "Other Languages";
          otherVoices.forEach((voice) => {
            const option = document.createElement("option");
            option.value = voices.indexOf(voice);
            option.textContent = `${voice.name} (${voice.lang})`;
            optgroup.appendChild(option);
          });
          voiceSelect.appendChild(optgroup);
        }
      }

      loadVoices();
      if (synth.addEventListener) {
        synth.addEventListener("voiceschanged", loadVoices);
      }

      function updateProgress(current, total) {
        const percent = Math.min(100, Math.round((current / total) * 100));
        progressText.textContent = `${percent}%`;
        progressBar.style.width = `${percent}%`;
        progressBar.setAttribute("aria-valuenow", percent);
      }

      function updatePlayButton(isPlaying) {
        const icon = playBtn.querySelector("i");
        if (isPlaying) {
          icon.className = "fas fa-circle-dot";
          playText.textContent = "Playing";
          playBtn.classList.add("playing");
        } else {
          icon.className = "fas fa-play";
          playText.textContent = "Play";
          playBtn.classList.remove("playing");
        }
      }

      function speak() {
        if (isPaused) {
          synth.resume();
          isPaused = false;
          updatePlayButton(true);
          pauseBtn.style.display = "inline-flex";
          return;
        }

        synth.cancel();
        utterance = new SpeechSynthesisUtterance(articleText);

        if (voiceSelect.value) {
          const voices = synth.getVoices();
          utterance.voice = voices[parseInt(voiceSelect.value)];
        }

        utterance.rate = parseFloat(rateSlider.value);
        utterance.pitch = 1;
        utterance.volume = 1;

        utterance.onstart = function () {
          updatePlayButton(true);
          pauseBtn.style.display = "inline-flex";
          playBtn.disabled = false;
        };

        utterance.onend = function () {
          updatePlayButton(false);
          pauseBtn.style.display = "none";
          updateProgress(100, 100);
          setTimeout(() => updateProgress(0, 100), 1000);
        };

        utterance.onboundary = function (event) {
          updateProgress(event.charIndex, articleText.length);
        };

        utterance.onerror = function (event) {
          if (event.error !== "interrupted" && event.error !== "canceled") {
            alert("Playback error. Try a different voice.");
          }
          updatePlayButton(false);
          pauseBtn.style.display = "none";
        };

        playBtn.disabled = true;
        synth.speak(utterance);
      }

      function pause() {
        if (synth.speaking && !isPaused) {
          synth.pause();
          isPaused = true;
          const icon = playBtn.querySelector("i");
          icon.className = "fas fa-play";
          playText.textContent = "Resume";
          playBtn.classList.remove("playing");
          pauseBtn.style.display = "none";
        }
      }

      function stop() {
        synth.cancel();
        isPaused = false;
        updatePlayButton(false);
        pauseBtn.style.display = "none";
        updateProgress(0, 100);
      }

      playBtn.addEventListener("click", speak);
      pauseBtn.addEventListener("click", pause);
      stopBtn.addEventListener("click", stop);

      rateSlider.addEventListener("input", function (e) {
        rateValue.textContent = `${parseFloat(e.target.value).toFixed(1)}x`;
      });

      window.addEventListener("beforeunload", function () {
        synth.cancel();
      });

      if (localStorage.getItem("audioPlayerExpanded") === "true") {
        setTimeout(() => {
          expandedPlayer.classList.add("show");
          triggerBtn.classList.add("active");
        }, 100);
      }
    }
  })();
</script> <article class="post-content"> <div id="markdown-content"> <blockquote> <p><strong>Update (June 2025):</strong> Since writing this post, I’ve expanded the repo to cover <a href="#beyond-llms-diffusion-models">diffusion model efficiency</a> as well - CFG caching, step distillation, and video latent caching. The core ideas about memory bandwidth translate surprisingly well.</p> </blockquote> <blockquote> <p><strong>Part 2:</strong> If you want to go deeper on the memory-bound problem, I wrote a follow-up on <a href="/blog/2025/triton-kernels-llm-inference/">writing custom Triton kernels</a> - going from 11% to 88% peak bandwidth on operations like RMSNorm.</p> </blockquote> <p>I’ve been obsessing over LLM inference efficiency lately. Not the flashy stuff like new architectures or training tricks - the unglamorous work of making inference <em>fast</em>. After spending a few weeks <a href="https://github.com/bassrehab/speculative-decoding" rel="external nofollow noopener" target="_blank">implementing speculative decoding from scratch</a>, I wanted to share what I learned. This covers both intuition and implementation details.</p> <h2 id="the-problem-that-kept-bugging-me">The Problem That Kept Bugging Me</h2> <p>Here’s what bothered me about LLM inference: we’re loading billions of parameters from memory just to generate a single token. Then we do it again. And again. For every single token.</p> <p>The arithmetic doesn’t make sense when you think about it. A 7B parameter model in float16 is about 14GB. On an A100 with 2TB/s memory bandwidth, just <em>loading</em> those weights takes ~7ms. But the actual computation? Maybe 0.1ms worth of FLOPs. We’re spending 98% of our time waiting for memory.</p> <p>This is what people mean when they say LLM inference is “memory-bound.” The GPU is mostly sitting idle, waiting for data.</p> <h3 id="visualizing-the-bottleneck-roofline-analysis">Visualizing the Bottleneck: Roofline Analysis</h3> <p>A roofline plot makes this concrete. The x-axis shows <strong>arithmetic intensity</strong> (FLOPs per byte of data moved), and the y-axis shows achievable performance. The diagonal line represents memory-bound workloads; the horizontal line represents compute-bound workloads.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/roofline-480.webp 480w,/assets/img/blog/roofline-800.webp 800w,/assets/img/blog/roofline-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blog/roofline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Roofline analysis showing LLM inference deep in the memory-bound region. All models cluster at AI ≈ 1, far below the ridge point.</figcaption> </figure> <p>The key numbers:</p> <ul> <li> <strong>Ridge point</strong>: ~30-150 FLOP/byte (depends on hardware)</li> <li> <strong>LLM inference</strong>: ~1 FLOP/byte (for FP16 autoregressive decoding)</li> </ul> <p>We’re <strong>two orders of magnitude</strong> below the ridge point. The GPU’s compute units are sitting idle 98% of the time, waiting for weights to load from memory. This is why speculative decoding works - if we’re paying the memory tax anyway, we might as well verify multiple tokens per load.</p> <blockquote> <p>For the full roofline analysis code and interactive benchmarks, see <a href="https://github.com/bassrehab/speculative-decoding/blob/main/roofline.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">roofline.py</code></a> in the repo.</p> </blockquote> <h2 id="the-insight-behind-speculative-decoding">The Insight Behind Speculative Decoding</h2> <p>The key insight from the <a href="https://arxiv.org/abs/2211.17192" rel="external nofollow noopener" target="_blank">Leviathan et al. paper</a> is surprisingly simple: what if we could verify multiple tokens in one forward pass instead of generating them one at a time?</p> <p>Think about it. If loading model weights dominates the cost, then processing 5 tokens costs almost the same as processing 1 token. We’re already paying the memory bandwidth tax - might as well do more work while we’re at it.</p> <p>The trick is using a small “draft” model to guess what tokens the big “target” model would generate. The draft model is fast because it’s small. Then we verify all those guesses in parallel with the big model. If the guesses are good, we just saved a bunch of forward passes.</p> <h2 id="how-it-actually-works">How It Actually Works</h2> <p>The algorithm has three phases that repeat:</p> <p><strong>Phase 1: Draft.</strong> Run the small model autoregressively for K tokens. This is fast because the model is tiny. I used GPT-2 (124M params) as my draft model.</p> <p><strong>Phase 2: Verify.</strong> Feed all K draft tokens to the big model in one forward pass. We get logits for every position.</p> <p><strong>Phase 3: Accept/Reject.</strong> Here’s where rejection sampling comes in. For each draft token, we compare the target model’s probability to the draft model’s probability. Accept with probability min(1, p_target/p_draft). If we reject, resample from an adjusted distribution and stop.</p> <p>Here’s the flow:</p> <pre><code class="language-mermaid">sequenceDiagram
    participant P as Prompt
    participant D as Draft Model (GPT-2)
    participant T as Target Model (GPT-2-L)
    participant O as Output

    P-&gt;&gt;D: "The weather is"

    Note over D: Phase 1: Draft K=4 tokens
    D-&gt;&gt;D: → "very"
    D-&gt;&gt;D: → "nice"
    D-&gt;&gt;D: → "today"
    D-&gt;&gt;D: → "."

    D-&gt;&gt;T: ["very", "nice", "today", "."]

    Note over T: Phase 2: Verify (single forward pass)
    T-&gt;&gt;T: Compute logits for all 4 positions

    Note over T: Phase 3: Accept/Reject
    T-&gt;&gt;O: ✓ Accept "very" (p_target ≥ p_draft)
    T-&gt;&gt;O: ✓ Accept "nice"
    T-&gt;&gt;O: ✗ Reject "today" → resample "pleasant"

    Note over O: Output: "very nice pleasant"&lt;br/&gt;3 tokens in ~1 target forward pass!
</code></pre> <p>The beautiful part is the math guarantees we get <em>exactly</em> the same distribution as running the target model alone. No approximation. No quality loss. Just faster.</p> <p>Here’s the core loop from <a href="https://github.com/bassrehab/speculative-decoding/blob/main/speculative_decoding.py#L363" rel="external nofollow noopener" target="_blank">my implementation</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="n">draft_token</span> <span class="o">=</span> <span class="n">draft_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">p_target</span> <span class="o">=</span> <span class="n">target_probs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">draft_token</span><span class="p">]</span>
    <span class="n">p_draft</span> <span class="o">=</span> <span class="n">draft_probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="c1"># Rejection sampling
</span>    <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="nf">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_target</span> <span class="o">/</span> <span class="n">p_draft</span><span class="p">):</span>
        <span class="n">accepted</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">draft_token</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Resample from adjusted distribution
</span>        <span class="n">adjusted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">target_probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">p_draft</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">adjusted</span> <span class="o">=</span> <span class="n">adjusted</span> <span class="o">/</span> <span class="n">adjusted</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="n">resampled</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">adjusted</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">accepted</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">resampled</span><span class="p">)</span>
        <span class="k">break</span>
</code></pre></div></div> <h2 id="what-i-found-surprising">What I Found Surprising</h2> <h3 id="greedy-beats-sampling-for-speed">Greedy beats sampling (for speed)</h3> <p>I expected sampling mode to work fine. It doesn’t - at least not for speed. With temperature sampling, the acceptance rate tanks because the draft and target models make different random choices even when their distributions are similar.</p> <p>Greedy decoding gave me 1.10x speedup. Sampling gave me 0.85x (actually slower than baseline). That was counterintuitive at first, but makes sense: with greedy, if both models agree on the argmax, we accept. With sampling, we need the random draws to align too.</p> <table> <thead> <tr> <th>Mode</th> <th>Speedup</th> <th>Acceptance Rate</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td>Greedy</td> <td>1.10x</td> <td>62.5%</td> <td>Both models pick same argmax → high acceptance</td> </tr> <tr> <td>Sampling</td> <td>0.85x</td> <td>64.3%</td> <td>Random draws must align → rejection overhead</td> </tr> <tr> <td>Adaptive K</td> <td>0.99x</td> <td>82.0%</td> <td>Higher acceptance, but more draft overhead per batch</td> </tr> </tbody> </table> <h3 id="the-acceptance-rate-formula-is-elegant">The acceptance rate formula is elegant</h3> <p>The probability of accepting a draft token works out to:</p> \[\alpha = \sum_x \min(p(x), q(x)) = 1 - D_{TV}(p, q)\] <p>It’s literally 1 minus the total variation distance between the two distributions. The closer your draft model is to the target, the higher your acceptance rate. Simple, but I had to work through the proof to believe it.</p> <details> <summary><strong>Proof sketch (click to expand)</strong></summary> The rejection sampling step accepts token $x$ with probability $\min(1, p(x)/q(x))$ where $p$ is target, $q$ is draft. The overall acceptance probability is: $$\alpha = \sum_x q(x) \cdot \min\left(1, \frac{p(x)}{q(x)}\right) = \sum_x \min(q(x), p(x))$$ This equals $1 - D_{TV}(p,q)$ by definition of total variation distance. The beautiful part: when we reject and resample from $\text{norm}(\max(0, p-q))$, the combined distribution is exactly $p$. No approximation. For the full derivation with the expected tokens formula, see [THEORY.md](https://github.com/bassrehab/speculative-decoding/blob/main/docs/THEORY.md#the-math-why-it-works). </details> <h3 id="kv-cache-management-is-annoying">KV-cache management is annoying</h3> <p>Nobody talks about this, but managing the KV-cache when tokens get rejected is a pain. You need to trim the cache back to the accepted length, and different models have different cache APIs. I spent more time on cache trimming than on the actual algorithm.</p> <h2 id="trying-different-approaches">Trying Different Approaches</h2> <p>After getting basic speculative decoding working, I implemented two variations:</p> <h3 id="medusa-parallel-drafting">Medusa: Parallel Drafting</h3> <p>Instead of drafting tokens one at a time, what if we predicted multiple positions in parallel? <a href="https://arxiv.org/abs/2401.10774" rel="external nofollow noopener" target="_blank">Medusa</a> adds extra “heads” to predict positions +1, +2, +3, etc. from the same hidden state.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph Base["Base Model"]
        H["Hidden State h_t"]
    end

    subgraph Heads["Medusa Heads"]
        H --&gt; H1["Head 1"]
        H --&gt; H2["Head 2"]
        H --&gt; H3["Head 3"]
        H --&gt; H4["Head 4"]
    end

    H1 --&gt; T1["t+1"]
    H2 --&gt; T2["t+2"]
    H3 --&gt; T3["t+3"]
    H4 --&gt; T4["t+4"]

    style H1 fill:#e1f5fe
    style H2 fill:#e1f5fe
    style H3 fill:#e1f5fe
    style H4 fill:#e1f5fe
</code></pre> <p>The upside: one forward pass drafts multiple tokens. The downside: the heads need training, and without that, acceptance rates are terrible. My randomly-initialized heads got ~34% acceptance. Not great.</p> <h3 id="eagle-using-target-features">EAGLE: Using Target Features</h3> <p><a href="https://arxiv.org/abs/2401.15077" rel="external nofollow noopener" target="_blank">EAGLE</a> is clever. Instead of using a separate draft model, it uses the target model’s own hidden states to predict future tokens. The draft head sees rich features from the big model, so it makes better predictions.</p> <pre><code class="language-mermaid">flowchart TB
    subgraph Target["Target Model"]
        I["Input"] --&gt; TM["Transformer Layers"]
        TM --&gt; HS["Hidden States"]
        TM --&gt; L["Logits"]
    end

    subgraph Draft["EAGLE Draft Head"]
        HS --&gt; |"Rich features"| DH["Lightweight\nTransformer"]
        TE["Token\nEmbedding"] --&gt; DH
        DH --&gt; DL["Draft Logits"]
    end

    DL --&gt; |"Draft tokens"| V["Verify with Target"]

    style HS fill:#c8e6c9
    style DH fill:#fff9c4
</code></pre> <p>I implemented a basic version with a small transformer on top of the target’s hidden states. Even without training, it shows promise. The architecture makes sense - you’re conditioning on information the target model already computed.</p> <h2 id="memory-the-other-bottleneck">Memory: The Other Bottleneck</h2> <p>While working on this, I went down a rabbit hole on KV-cache compression. For long sequences, the cache becomes massive. LLaMA-7B at 32K context needs 8+ GB just for the KV-cache.</p> <p>I implemented a few compression strategies:</p> <ul> <li> <strong>H2O eviction</strong>: Keep “heavy hitter” tokens (high attention) plus recent tokens. Works surprisingly well.</li> <li> <strong>Sliding window</strong>: Keep the last N tokens plus a few “attention sink” tokens at the start.</li> <li> <strong>INT8 quantization</strong>: 2x memory reduction with negligible quality loss (~0.5% reconstruction error).</li> </ul> <p>The combination of eviction + quantization can get you 8x compression. That’s the difference between fitting in VRAM or not.</p> <h2 id="limitations-and-honest-assessment">Limitations and Honest Assessment</h2> <p>My benchmarks show modest speedups (1.1x) on GPT-2. That’s… fine. Not amazing. Here’s why:</p> <ol> <li> <p><strong>GPT-2 is small.</strong> The draft-to-target ratio matters. GPT-2 to GPT-2-Large is only 6x. Papers showing 2-3x speedup use 70B targets with 7B drafts.</p> </li> <li> <p><strong>MPS backend variance.</strong> Running on my M1 Mac, I saw high variance (±10% per run). CUDA numbers are more stable.</p> </li> <li> <p><strong>Memory bandwidth differences.</strong> The whole premise assumes memory-bound inference. On smaller models, you might be compute-bound instead.</p> </li> <li> <p><strong>Batch size &gt; 1.</strong> Speculative decoding helps most at batch size 1. With larger batches, you’re already utilizing compute better (more arithmetic intensity), and the draft/verify overhead becomes less attractive. For high-throughput serving with batching, other optimizations (continuous batching, paged attention) often matter more.</p> </li> </ol> <p>If you’re running LLaMA-70B on A100s, you’ll see much better speedups. If you’re running GPT-2 on a laptop, maybe don’t bother.</p> <h2 id="beyond-llms-diffusion-models">Beyond LLMs: Diffusion Models</h2> <p>The memory-bound insight from the <a href="#visualizing-the-bottleneck-roofline-analysis">roofline analysis</a> above isn’t unique to LLMs - it applies anywhere you’re loading massive model weights for each forward pass. Diffusion models are a prime example: instead of one forward pass per token, you have 20-1000 denoising steps per image, each loading the full UNet weights.</p> <p>I implemented a few techniques in <a href="https://github.com/bassrehab/speculative-decoding/blob/main/diffusion_efficiency.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">diffusion_efficiency.py</code></a>:</p> <p><strong>CFG Caching.</strong> Classifier-free guidance requires two forward passes per step: conditional and unconditional. But the unconditional prediction changes slowly. Cache it, reuse it every other step, and you skip ~50% of those forward passes. I measured 3.3x speedup on a toy UNet.</p> <p><strong>Step Distillation.</strong> The idea from <a href="https://arxiv.org/abs/2202.00512" rel="external nofollow noopener" target="_blank">Salimans &amp; Ho</a>: train a student to match two teacher steps in one. Repeat until you’re down from 1000 steps to 4. Each halving needs training, but the final model is dramatically faster.</p> <p><strong>Video Latent Caching.</strong> Video diffusion is brutal - every frame needs full denoising. But frames are correlated. Cache keyframes, interpolate the rest (slerp works better than linear for latent spaces), then refine with fewer steps. Same idea as I-frames and P-frames in video codecs.</p> <p>These aren’t as elegant as speculative decoding’s rejection sampling guarantee, but they’re practical. The field is moving fast here - consistency models, rectified flows, and other approaches are changing what’s possible.</p> <h2 id="what-id-do-differently">What I’d Do Differently</h2> <p>A few things I’d change if starting over:</p> <ul> <li> <strong>Start with bigger models.</strong> Testing on GPT-2 was convenient but didn’t show the real benefits.</li> <li> <strong>Try tree speculation earlier.</strong> Generating a tree of candidates instead of a single sequence is more complex but potentially faster. I implemented this in <a href="https://github.com/bassrehab/speculative-decoding/blob/main/tree_speculation.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">tree_speculation.py</code></a> and it does help with acceptance rates.</li> </ul> <h2 id="code">Code</h2> <p>Everything is on GitHub: <a href="https://github.com/bassrehab/speculative-decoding" rel="external nofollow noopener" target="_blank">github.com/bassrehab/speculative-decoding</a></p> <p>The main files:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">speculative_decoding.py</code> - Core implementation with KV-cache</li> <li> <code class="language-plaintext highlighter-rouge">tree_speculation.py</code> - Tree-based candidate exploration</li> <li> <code class="language-plaintext highlighter-rouge">eagle.py</code> - EAGLE-style feature-aware drafting</li> <li> <code class="language-plaintext highlighter-rouge">medusa.py</code> - Multi-head parallel drafting</li> <li> <code class="language-plaintext highlighter-rouge">kv_cache_compression.py</code> - Cache eviction and quantization</li> <li> <code class="language-plaintext highlighter-rouge">diffusion_efficiency.py</code> - CFG caching, step distillation, video latent caching</li> <li> <code class="language-plaintext highlighter-rouge">roofline.py</code> - Memory bandwidth profiling and roofline analysis</li> </ul> <p>There’s also proper documentation in <a href="https://github.com/bassrehab/speculative-decoding/blob/main/docs/THEORY.md" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">docs/THEORY.md</code></a> if you want the full mathematical treatment, and an interactive <a href="https://github.com/bassrehab/speculative-decoding/blob/main/demo.ipynb" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">demo.ipynb</code></a> notebook.</p> <h2 id="takeaways">Takeaways</h2> <p>Speculative decoding is elegant. The rejection sampling guarantee means you get exact target distribution with zero approximation - just faster. But the speedup depends heavily on your setup.</p> <p>The real insight isn’t the algorithm itself. It’s the shift in thinking from “generate tokens sequentially” to “verify tokens in parallel.” That mental model opens up a lot of optimization opportunities.</p> <p>If you’re working on LLM inference, understanding memory bandwidth is more important than understanding FLOPs. The bottleneck isn’t compute anymore.</p> <hr> <p><em>Questions or thoughts? Feel free to reach out or open an issue on the repo.</em></p> </div> </article> <div class="citation-section"> <div class="citation-header"> <h3>Citation</h3> <p class="citation-intro">If you found this article useful, please cite it using one of the formats below:</p> </div> <div class="citation-content"> <div class="citation-format"> <h4>APA Format</h4> <div class="citation-text"> <p> Mitra, Subhadip. (2025, March). <em>Making LLMs Faster: My Deep Dive into Speculative Decoding</em>. Retrieved from https://subhadipmitra.com/blog/2025/making-llm-faster/ </p> </div> </div> <div class="citation-format"> <h4>BibTeX Entry</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">mitra2025making-llms-faster-my-deep-dive-into-speculative-decoding</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Making LLMs Faster: My Deep Dive into Speculative Decoding}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Mitra, Subhadip}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Mar}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://subhadipmitra.com/blog/2025/making-llm-faster/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> </div> <style>.post-footer-featured{margin-top:6rem;padding-top:0}.newsletter-featured{background:var(--global-card-bg-color,#fff);border:2px solid var(--global-theme-color,#b509ac);border-radius:24px;padding:3rem;margin-bottom:3rem}.showcase-label-newsletter{display:inline-flex;align-items:center;gap:.5rem;font-size:.875rem;letter-spacing:.15em;text-transform:uppercase;color:var(--global-theme-color,#b509ac);font-weight:700;margin-bottom:1rem}.newsletter-featured h2{font-size:2rem;font-weight:900;margin:0 0 1rem 0;letter-spacing:-0.02em;line-height:1.1;color:var(--global-text-color,#000)}.newsletter-featured p{font-size:1.125rem;color:var(--global-text-color-light,#666);margin-bottom:1.5rem;line-height:1.6}.newsletter-form-featured{display:flex;gap:.75rem;margin-bottom:1rem}.newsletter-form-featured input{flex:1;padding:.875rem 1.25rem;background:var(--global-bg-color,#fff);color:var(--global-text-color,#000);border:1px solid var(--global-divider-color,#e5e5e5);border-radius:12px;font-size:1rem;transition:all .3s}.newsletter-form-featured input:focus{outline:0;border-color:var(--global-theme-color,#b509ac);box-shadow:0 0 0 3px rgba(181,9,172,0.1)}.newsletter-form-featured button{padding:.875rem 2rem;background:var(--global-theme-color,#b509ac);color:white;border:0;border-radius:12px;font-weight:700;font-size:1rem;cursor:pointer;transition:all .3s;white-space:nowrap}.newsletter-form-featured button:hover{transform:translateY(-4px);box-shadow:0 12px 40px rgba(181,9,172,0.3)}.newsletter-links{display:flex;gap:1rem;flex-wrap:wrap;align-items:center;font-size:.875rem;color:var(--global-text-color-light,#666)}.newsletter-links a{color:var(--global-theme-color,#b509ac);text-decoration:none;font-weight:600;transition:opacity .3s}.newsletter-links a:hover{opacity:.7}.share-section-featured{text-align:center;padding:2rem 0 3rem 0;border-bottom:1px solid var(--global-divider-color,#e5e5e5)}.share-section-featured .share-label{font-size:.875rem;text-transform:uppercase;letter-spacing:.1em;color:var(--global-text-color-light,#666);font-weight:600;margin-bottom:1.25rem}.share-buttons-featured{display:flex;gap:1rem;justify-content:center;align-items:center}.share-link-featured{display:inline-flex;align-items:center;gap:.5rem;font-weight:600;color:var(--global-text-color,#000);text-decoration:none;font-size:.875rem;padding:.5rem 1rem;background:var(--global-code-bg-color,#f8f9fa);border:1px solid var(--global-divider-color,#e5e5e5);border-radius:8px;transition:all .2s ease;white-space:nowrap;cursor:pointer}.share-link-featured:hover{background:var(--global-theme-color,#b509ac);border-color:var(--global-theme-color,#b509ac);color:white;transform:translateY(-2px)}.share-link-featured:hover svg{fill:white}.share-link-featured svg{width:16px;height:16px;flex-shrink:0;transition:fill .2s ease}.related-posts-section{margin-top:3rem;padding-top:0}.related-posts-section h3{font-size:1.75rem;font-weight:800;margin-bottom:2rem;color:var(--global-text-color,#000);text-align:center}@media(max-width:768px){.newsletter-featured{padding:2rem}.newsletter-featured h2{font-size:1.5rem}.newsletter-form-featured{flex-direction:column}.share-buttons-featured{flex-wrap:wrap;gap:.5rem}}</style> <div class="post-footer-featured"> <div class="newsletter-featured"> <div class="showcase-label-newsletter"> <svg width="16" height="16" viewbox="0 0 24 24" fill="currentColor"> <path d="M12 2l3.09 6.26L22 9.27l-5 4.87 1.18 6.88L12 17.77l-6.18 3.25L7 14.14 2 9.27l6.91-1.01L12 2z"></path> </svg> Strategic Insights </div> <h2>Get More Like This</h2> <p> Join technical leaders from Google, Amazon, and Fortune 500s. Get strategic insights on Data, AI, and Cloud transformation delivered to your inbox. </p> <form class="newsletter-form-featured" action="https://app.loops.so/api/newsletter-form/cm614n2d604nlfy1lfo7vgmo5" method="POST"> <input type="email" name="email" placeholder="your@email.com" required> <button type="submit">Subscribe</button> </form> <div class="newsletter-links"> <span>Free insights • No spam • Unsubscribe anytime</span> <span>•</span> <a href="/archive/">Browse the archive →</a> </div> </div> <div class="share-section-featured"> <div class="share-label">Share This Article</div> <div class="share-buttons-featured"> <a href="https://twitter.com/intent/tweet?text=Making%20LLMs%20Faster:%20My%20Deep%20Dive%20into%20Speculative%20Decoding&amp;url=https://subhadipmitra.com/blog/2025/making-llm-faster/&amp;via=bassrehab" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on X"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.6823 10.6218L20.2391 3H18.6854L12.9921 9.61788L8.44486 3H3.2002L10.0765 13.0074L3.2002 21H4.75404L10.7663 14.0113L15.5685 21H20.8131L13.6819 10.6218H13.6823ZM11.5541 13.0956L10.8574 12.0991L5.31391 4.16971H7.70053L12.1742 10.5689L12.8709 11.5655L18.6861 19.8835H16.2995L11.5541 13.096V13.0956Z"></path> </svg> <span>X</span> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://subhadipmitra.com/blog/2025/making-llm-faster/" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on LinkedIn"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path> </svg> <span>LinkedIn</span> </a> <a href="https://www.reddit.com/submit?url=https://subhadipmitra.com/blog/2025/making-llm-faster/&amp;title=Making%20LLMs%20Faster:%20My%20Deep%20Dive%20into%20Speculative%20Decoding" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on Reddit"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M12 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 0 1-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 0 1 .042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 0 1 4.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 0 1 .14-.197.35.35 0 0 1 .238-.042l2.906.617a1.214 1.214 0 0 1 1.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 0 0-.231.094.33.33 0 0 0 0 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 0 0 .029-.463.33.33 0 0 0-.464 0c-.547.533-1.684.73-2.512.73-.828 0-1.979-.196-2.512-.73a.326.326 0 0 0-.232-.095z"></path> </svg> <span>Reddit</span> </a> <a href="https://news.ycombinator.com/submitlink?u=https://subhadipmitra.com/blog/2025/making-llm-faster/&amp;t=Making%20LLMs%20Faster:%20My%20Deep%20Dive%20into%20Speculative%20Decoding" class="share-link-featured" target="_blank" rel="noopener noreferrer" title="Share on Hacker News"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M0 0v24h24V0H0zm12.3 12.6l-3.6-7.2H10l2.4 5.1 2.4-5.1h1.3l-3.6 7.2v4.8h-1.2v-4.8z"></path> </svg> <span>HN</span> </a> <a href="mailto:?subject=Making%20LLMs%20Faster:%20My%20Deep%20Dive%20into%20Speculative%20Decoding&amp;body=Check%20out%20this%20article:%20https://subhadipmitra.com/blog/2025/making-llm-faster/" class="share-link-featured" title="Share via Email"> <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"></path> </svg> <span>Email</span> </a> <button onclick="copyLinkFeatured('https://subhadipmitra.com/blog/2025/making-llm-faster/')" class="share-link-featured" title="Copy link to clipboard"> <svg class="copy-icon" viewbox="0 0 24 24" fill="currentColor"> <path d="M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z"></path> </svg> <svg class="check-icon" style="display: none" viewbox="0 0 24 24" fill="currentColor"> <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path> </svg> <span class="copy-text">Copy</span> <span class="copied-text" style="display: none">Copied!</span> </button> </div> </div> <div class="related-posts-section"> <h3>Continue Reading</h3> </div> </div> <script>
  function copyLinkFeatured(url) {
    navigator.clipboard
      .writeText(url)
      .then(() => {
        const btn = event.target.closest("button");
        const copyText = btn.querySelector(".copy-text");
        const copiedText = btn.querySelector(".copied-text");
        const copyIcon = btn.querySelector(".copy-icon");
        const checkIcon = btn.querySelector(".check-icon");

        copyText.style.display = "none";
        copiedText.style.display = "inline";
        copyIcon.style.display = "none";
        checkIcon.style.display = "inline";

        setTimeout(() => {
          copyText.style.display = "inline";
          copiedText.style.display = "none";
          copyIcon.style.display = "inline";
          checkIcon.style.display = "none";
        }, 2000);
      })
      .catch((err) => {
        console.error("Failed to copy:", err);
        alert("Could not copy link. Please copy manually: " + url);
      });
  }

  // Newsletter form handler
  (function () {
    const form = document.querySelector(".newsletter-form-featured");
    if (!form) return;

    form.addEventListener("submit", function (event) {
      event.preventDefault();

      const emailInput = form.querySelector('input[name="email"]');
      const submitBtn = form.querySelector('button[type="submit"]');
      const email = emailInput.value;

      // Disable submit button and show loading state
      submitBtn.disabled = true;
      const originalText = submitBtn.textContent;
      submitBtn.textContent = "Subscribing...";

      // Submit to Loops.so
      const formBody = "userGroup=&email=" + encodeURIComponent(email);
      fetch(form.action, {
        method: "POST",
        body: formBody,
        headers: {
          "Content-Type": "application/x-www-form-urlencoded",
        },
      })
        .then((res) => res.json().then((data) => ({ ok: res.ok, data })))
        .then(({ ok, data }) => {
          if (ok) {
            // Success - show confirmation message
            submitBtn.textContent = "✓ Subscribed!";
            submitBtn.style.background = "#22c55e";
            emailInput.value = "";

            // Reset after 3 seconds
            setTimeout(() => {
              submitBtn.textContent = originalText;
              submitBtn.style.background = "";
              submitBtn.disabled = false;
            }, 3000);
          } else {
            // Error from API
            throw new Error(data.message || "Subscription failed");
          }
        })
        .catch((error) => {
          // Show error
          submitBtn.textContent = "✗ Failed";
          submitBtn.style.background = "#ef4444";
          console.error("Newsletter subscription error:", error);

          // Reset after 3 seconds
          setTimeout(() => {
            submitBtn.textContent = originalText;
            submitBtn.style.background = "";
            submitBtn.disabled = false;
          }, 3000);
        });
    });
  })();
</script> <style>.related-posts-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:1.5rem}.related-post-card{background:var(--global-card-bg-color,#fff);border:1px solid var(--global-divider-color,#e5e5e5);border-radius:16px;padding:2rem;transition:all .3s cubic-bezier(0.4,0,0.2,1);position:relative;display:flex;flex-direction:column}.related-post-card:hover{transform:translateY(-4px);box-shadow:0 12px 32px rgba(0,0,0,0.1);border-color:var(--global-theme-color,#b509ac)}.related-post-title{font-size:1.25rem;font-weight:700;line-height:1.3;margin:0 0 .75rem 0;color:var(--global-text-color,#000)}.related-post-title a{color:inherit;text-decoration:none;transition:color .3s}.related-post-title a:hover{color:var(--global-theme-color,#b509ac)}.related-post-meta{font-size:.875rem;color:var(--global-text-color-light,#666);margin-bottom:.75rem}.related-post-excerpt{font-size:.95rem;line-height:1.6;color:var(--global-text-color-light,#666);margin-bottom:1rem;flex-grow:1}.related-post-link{display:inline-flex;align-items:center;gap:.5rem;font-weight:700;color:var(--global-theme-color,#b509ac);text-decoration:none;font-size:.95rem;transition:gap .3s;margin-top:auto}.related-post-link:hover{gap:1rem}.related-post-link svg{width:16px;height:16px;transition:transform .3s}.related-post-link:hover svg{transform:translateX(4px)}.external-link-icon{width:14px;height:14px;opacity:.6;margin-left:.25rem}.no-related-posts{text-align:center;padding:3rem;color:var(--global-text-color-light,#666);font-style:italic}@media(max-width:768px){.related-posts-grid{grid-template-columns:1fr;gap:1rem}.related-post-card{padding:1.5rem}.related-post-title{font-size:1.125rem}}</style> <div class="related-posts-container"> <div class="related-posts-grid"> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2026/moltbook-mcp-stress-test/">Moltbook as MCP Stress Test: What 770K Agents Reveal About Protocol Design</a> </h4> <div class="related-post-meta"> February 02, 2026 • moltbook </div> <p class="related-post-excerpt">A follow-up to my MCP Maturity Model post. Moltbook shows what happens when you run 770K agents at Level 0 maturity with zero governance. The...</p> <a href="/blog/2026/moltbook-mcp-stress-test/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2026/circuit-tracing-production/">Circuit Tracing for the Rest of Us: From Probes to Attribution Graphs and What It Means for Production Safety</a> </h4> <div class="related-post-meta"> January 31, 2026 • mechanistic-interpretability </div> <p class="related-post-excerpt">MIT Tech Review named mechanistic interpretability a 2026 Breakthrough Technology. Anthropic open-sourced circuit tracing. Here's what actually changed, how it connects to the activation probes...</p> <a href="/blog/2026/circuit-tracing-production/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2026/rlvr-beyond-math-code/">RLVR Beyond Math and Code: The Verifier Problem Nobody Has Solved</a> </h4> <div class="related-post-meta"> January 18, 2026 • RLVR </div> <p class="related-post-excerpt">Reinforcement Learning with Verifiable Rewards powers every reasoning model worth talking about. But it only works where you can check the answer automatically. Extending it...</p> <a href="/blog/2026/rlvr-beyond-math-code/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> <article class="related-post-card"> <h4 class="related-post-title"> <a href="/blog/2026/agent-protocol-stack/">The Agent Protocol Stack: Why MCP + A2A + A2UI Is the TCP/IP Moment for Agentic AI</a> </h4> <div class="related-post-meta"> January 06, 2026 • MCP </div> <p class="related-post-excerpt">MCP handles agent-to-tool. A2A handles agent-to-agent. A2UI handles agent-to-interface. Together they form a protocol stack that nobody has mapped properly - including the security gaps...</p> <a href="/blog/2026/agent-protocol-stack/" class="related-post-link"> Read More <svg viewbox="0 0 24 24" fill="currentColor"> <path d="M13.025 1l-2.847 2.828 6.176 6.176h-16.354v3.992h16.354l-6.176 6.176 2.847 2.828 10.975-11z"></path> </svg> </a> </article> </div> </div> <br> <br> <div class="hyvor-talk-container"> <style>.hyvor-talk-container{margin-top:4rem;padding-top:3rem;border-top:2px solid var(--global-divider-color,#e5e5e5)}.hyvor-talk-header{margin-bottom:2rem;text-align:center}.hyvor-talk-header h3{font-size:1.75rem;font-weight:800;margin-bottom:.5rem;color:var(--global-text-color,#000)}.hyvor-talk-header p{font-size:1rem;color:var(--global-text-color-light,#666);margin:0}</style> <div class="hyvor-talk-header"> <h3>Join the Discussion</h3> <p>Share your thoughts, ask questions, or provide feedback on this article</p> </div> <div id="hyvor-talk-view"></div> <script type="application/javascript">
    var HYVOR_TALK_WEBSITE = 14339;
    var HYVOR_TALK_CONFIG = {
      url: 'https://subhadipmitra.com/blog/2025/making-llm-faster/',
      id: '/blog/2025/making-llm-faster/',
      
      title: 'Making LLMs Faster: My Deep Dive into Speculative Decoding',
      
      
      palette: {
        accent: '#b509ac',
        accentText: '#ffffff',
        footerHeader: 'var(--global-text-color)',
        footerHeaderText: 'var(--global-text-color-light)',
        box: 'var(--global-card-bg-color)',
        boxText: 'var(--global-text-color)',
        boxLightText: 'var(--global-text-color-light)',
        backgroundText: 'var(--global-text-color)'
      }
      
    };
  </script> <script async type="application/javascript" src="//talk.hyvor.com/web-api/embed.js"></script> </div> </div> </div> </div> </div> <footer class="modern-footer" role="contentinfo"> <div class="footer-container"> <div class="footer-grid"> <div class="footer-brand"> <h3>Subhadip Mitra</h3> <p> Technical Leader, Inventor, and Researcher building the future of Data &amp; Applied AI. <br>Leading Google Cloud's D&amp;A practice across Southeast Asia. </p> <br> <p><a href="mailto:contact@subhadipmitra.com">contact@subhadipmitra.com</a></p> </div> <div class="footer-section"> <h4>Explore</h4> <ul class="footer-links"> <li><a href="/">About</a></li> <li><a href="/now/">What I'm Doing Now</a></li> <li><a href="/blog/">Blog</a></li> <li><a href="/publications/">Publications</a></li> <li><a href="/repositories/">Repositories</a></li> </ul> </div> <div class="footer-section"> <h4>Connect</h4> <ul class="footer-links"> <li><a href="/contact/">Contact</a></li> <li><a href="mailto:contact@subhadipmitra.com">Email</a></li> <li><a href="https://calendly.com/contact-x9nm/30min" target="_blank" rel="external nofollow noopener">Schedule a Call</a></li> <li><a href="https://linkedin.com/in/subhadip-mitra" target="_blank" rel="external nofollow noopener">LinkedIn</a></li> <li><a href="https://github.com/bassrehab" target="_blank" rel="external nofollow noopener">GitHub</a></li> </ul> </div> </div> <div class="footer-bottom"> <div class="footer-copyright"> © 2026 Subhadip Mitra. Some Rights Reserved. Last updated: February 03, 2026. <span class="footer-utility-links"> <a href="/privacy/" title="Privacy Policy">Privacy</a> · <a href="/license/" title="Licenses">Licenses</a> · <a href="/sitemap.xml" title="Sitemap">Sitemap</a> · <a href="/feed.xml" title="RSS Feed">RSS</a> · <a href="/llms.txt" title="AI Agent Information">LLMs</a> </span> </div> <div class="footer-social"> <a href="https://linkedin.com/in/subhadip-mitra" target="_blank" aria-label="LinkedIn" rel="external nofollow noopener"> <i class="fab fa-linkedin-in"></i> </a> <a href="https://github.com/bassrehab" target="_blank" aria-label="GitHub" rel="external nofollow noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/bassrehab" target="_blank" aria-label="Twitter" rel="external nofollow noopener"> <i class="fab fa-twitter"></i> </a> <a href="mailto:contact@subhadipmitra.com" aria-label="Email"> <i class="fas fa-envelope"></i> </a> </div> </div> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="/assets/js/toc-toggle.js?9c8277d11f423c45ff70df63cbe75108"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?8e03581f97e30ec91fff7c9869d3c6cb"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-TW7YQ5XPC6"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-TW7YQ5XPC6');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <script defer src="/assets/js/newsletter.js?c3d0931971ee96e9df74ba70526c3130"></script> </body> </html>